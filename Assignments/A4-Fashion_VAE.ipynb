{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f0b33b-5a7a-43fd-b215-3af2812a525e",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "<br> \n",
    "    <center>\n",
    "        <img src=\"src/A4_fashion-mnist.png\" width=\"600\"/>\n",
    "    </center>\n",
    "</br>\n",
    "\n",
    "In the last assignment, you can develop your own **generative neural network** to create novel fashion items.\n",
    "For training, we consider the **Fashion-MNIST** dataset and you can work on\n",
    "\n",
    "* developing, training, and testing convolutional neural networks,\n",
    "* get an insight into **conditional** variational autoencoders,\n",
    "* and explore **embeddings**.\n",
    "\n",
    "For this exercise, you can switch back to the environment ```APML``` used during class.\n",
    "\n",
    "Please provide solutions to all exercises below and send me all notebooks by **31st of May 2024**.\n",
    "\n",
    "***\n",
    "\n",
    "## Part I: Fashion-MNIST label prediction\n",
    "\n",
    "In the classes, we have worked with the MNIST benchmark dataset of 70'000 images \n",
    "(usually 28 x 28 pixels) of hand-written digits. Zalando Research provides a similar\n",
    "but more challenging dataset with their [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)\n",
    "dataset of 70'000 images (28 x 28 pixels) of different fashion items belonging to the \n",
    "following ten classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b744599-e3f6-4b28-9e68-b5c8486e9379",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_labels = {0: 'T-shirt',\n",
    "                  1: 'Trouser',\n",
    "                  2: 'Pullover',\n",
    "                  3: 'Dress',\n",
    "                  4: 'Coat',\n",
    "                  5: 'Sandal',\n",
    "                  6: 'Shirt',\n",
    "                  7: 'Sneaker',\n",
    "                  8: 'Bag',\n",
    "                  9: 'Ankle boot'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d565e63a-984a-4e99-84b1-5fe695ce1af9",
   "metadata": {},
   "source": [
    "In order to load the dataset, let us define the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594c213-dde5-48e9-8e57-88d34ca2005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "def load_fashion_mnist(path, kind='train'):\n",
    "    labels_path = os.path.join(path,\n",
    "                               f'Fashion-MNIST_{kind}-labels.gz'\n",
    "                              )\n",
    "    images_path = os.path.join(path,\n",
    "                               f'Fashion-MNIST_{kind}-images.gz'\n",
    "                               )\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 28, 28, 1)\n",
    "\n",
    "    return images.astype(np.float32), labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23437be6-9f01-4e42-a197-55338e7ce249",
   "metadata": {},
   "source": [
    "and load the dataset in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1866ec04-d007-4e8b-b3c4-0433cb907bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train, label_train = load_fashion_mnist(path='data', kind='train')\n",
    "img_test, label_test = load_fashion_mnist(path='data', kind='test')\n",
    "\n",
    "print(f\"img_train.shape:\\t{img_train.shape}\")\n",
    "print(f\"label_train.shape:\\t{label_train.shape}\")\n",
    "print(f\"img_test.shape:\\t\\t{img_test.shape}\")\n",
    "print(f\"label_test.shape:\\t{label_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8e429-6031-47fc-a9c2-8d17e3d61d8a",
   "metadata": {},
   "source": [
    "### Exercise I.1\n",
    "\n",
    "First, let's get a feeling for the dataset. \n",
    "\n",
    "* Visualise the first 20 images of the **test set**. Plot all images as subplots in one big figure with four rows and five columns.\n",
    "* As the title for each subplot, set the label string provided in ```fashion_labels``` above.\n",
    "\n",
    "Hints: Remember that subplots were discussed in [Notebook 1](../Notebooks/1-Python_Concepts.ipynb). Your solution could include the following parts\n",
    "\n",
    "*  ```axs.ravel()```\n",
    "*  ```ax.axis('off')```\n",
    "*  ```cmap='binary_r'```\n",
    "\n",
    "Note that your own solution might not require all of these.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae32850-e9fb-4814-8097-37b0c35b3d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a932ab6d-2470-4372-b838-38cc81aa282c",
   "metadata": {},
   "source": [
    "### Exercise I.2\n",
    "\n",
    "Normalise the input images, such that pixel values are in the range between 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536fc8c-7b24-42aa-8056-3f2ceaca819d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa18b2cc-f7c9-482e-b20e-5ee390809b86",
   "metadata": {},
   "source": [
    "### Exercise I.3\n",
    "\n",
    "Train a convolutional neural network to predict the fashion item label for a\n",
    "new image. Define the following architecture:\n",
    "\n",
    "* 28 $ \\times $ 28 $ \\times $ 1 neurons in the input layer for the greyscale image\n",
    "* 6 conv. filters of size $3 \\times 3$ with ReLU activation followed by a $2 \\times 2$ max pooling\n",
    "* 16 conv. filters of size $3 \\times 3$ with ReLU activation followed by a $2 \\times 2$ max pooling\n",
    "* flatten these feature maps in a single vector \n",
    "* 1024 neurons in a dense layer with ReLU activation\n",
    "* 512 neurons in a dense layer with ReLU activation\n",
    "* 10 output neurons for our classes in the output layer with a softmax activation\n",
    "* use the ```sparse_categorical_crossentropy``` loss\n",
    "* store training results in ```fashion_cnn_history```\n",
    "* store weights in ```output/Fashion-MNIST_CNN_weights.h5```\n",
    "\n",
    "You can use the following hyperparameters but also explore other choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e2ee73-bc2e-4f1b-ada5-79094a641a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 16\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcebdff6-0b4d-4975-9215-37ed08f27c9c",
   "metadata": {},
   "source": [
    "Fill in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf668487-cefd-4a26-b266-1de22250f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.utils.set_random_seed(123)\n",
    "\n",
    "fashion_cnn = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a24e3f-7889-40a4-a9e0-af53e2188135",
   "metadata": {},
   "source": [
    "### Exercise I.4\n",
    "\n",
    "Visualise the training outcome:\n",
    "\n",
    "1. Use ```fashion_cnn_history``` to plot ```accuracy``` and ```loss``` for the training set and ```val_accuracy``` and ```val_loss``` for the test set. \n",
    "2. Reuse your plotting routine of ```Exercise I.1``` and add the predicted fashion label to the title of the 20 test images you visualised earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f53ea-dfa6-43e8-9b8d-b145f48904e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3f4c0-f197-4f84-9f19-80655e1c43f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdcd3ad2-49ff-4284-9f63-34150b01eab6",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Part II: Training a generative model\n",
    "\n",
    "In [Notebook 6](../Notebooks/6-VAE.ipynb), we have discussed the concept of\n",
    "variational autoencoders (VAEs) in unsupervised learning. There, we used an encoder to compress our input\n",
    "data into a latent representation $Z$, which is supposed to capture the relevant\n",
    "information of our input and allow us to reconstruct the original input. The following\n",
    "illustration depicts the general idea of the architecture:\n",
    "\n",
    "<br>\n",
    "    <center>\n",
    "        <img src=\"src/A4_VAE_illustration.png\" width=\"400\"/>\n",
    "    </center>\n",
    "</br>\n",
    "\n",
    "In this second part, we want to extend the standard VAE to the **conditional VAE**\n",
    "as illustrated below:\n",
    "\n",
    "<br>\n",
    "    <center>\n",
    "        <img src=\"src/A4_condVAE_illustration.png\" width=\"400\"/>\n",
    "    </center>\n",
    "</br>\n",
    "\n",
    "If labels are available, we can make use of these labels as additional inputs to\n",
    "the encoder and decoder.\n",
    "\n",
    "In this part, you can generate novel fashion items. With the hyperparameters specified below \n",
    "(in particular, after 20 training epochs and using 10 latent dimensions), you can expect\n",
    "your reconstructions for the first 20 images of the test set to look like the following:\n",
    "\n",
    "<br>\n",
    "    <center>\n",
    "        <img src=\"src/A4_VAE_test_reconstructions.png\" width=\"400\"/>\n",
    "    </center>\n",
    "</br>\n",
    "\n",
    "(Compare this to your plot of the first 20 images of the test set in Exercises I.1 and I.3)\n",
    "\n",
    "### There are two subparts:\n",
    "\n",
    "* **Subpart II.A**: In the first subpart, you are asked to implement the conditional VAE. This will require some extension of what you have encountered in the course. Please try to complete this subpart first. However, if you get stuck and cannot complete the implementation, please proceed with **subpart II.B**. Please keep your intermediate results and attempts in the coding cells! If you were successful with subpart II.A, **you can skip subpart** II.B.\n",
    "* **Subpart II.B**: In the second subpart, you can implement the standard VAE as encountered during the course in [Notebook 6](../Notebooks/6-VAE.ipynb). Please do this subpart only if you cannot implement the conditional VAE in subpart II.A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ece3ed-9d3d-43d1-9f29-37dc340edcc2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Subpart II.A - Fashion Conditional VAE\n",
    "\n",
    "In this subpart, you can try to implement the conditional VAE. You can reuse parts of your CNN in Exercise I.3 as the encoder.\n",
    "\n",
    "What you need to do:\n",
    "\n",
    "* Recall the VAE implementation covered in [Notebook 6](../Notebooks/6-VAE.ipynb). You can follow closely what we have used there.\n",
    "* The illustration above is supposed to give you an idea of the implementation. In particular, you will need to adjust the class methods ```encode(self, x)``` and ```decode(self, z, apply_sigmoid=False)``` to include the target labels ```y```.\n",
    "* You will need to adjust any other function and method which calls these functions to include the target labels ```y```, too.\n",
    "* Finally, your encoder and decoder definitions ```self.encoder``` and ```self.decoder``` will need an adjustment in the input layers to account for ```y```.\n",
    "\n",
    "With these changes, you should be able to implement the conditional VAE.\n",
    "\n",
    "Define the following encoder architecture:\n",
    "\n",
    "* Figure out what the input layer needs to look like. It should be a minor adjustment to what you have used in Exercise I.3.\n",
    "* 6 conv. filters of size $3 \\times 3$ with stride $2$ with ReLU activation followed by a $2 \\times 2$ max pooling\n",
    "* 16 conv. filters of size $3 \\times 3$ with stride $2$ with ReLU activation followed by a $2 \\times 2$ max pooling\n",
    "* flatten these feature maps in a single vector \n",
    "* 1024 neurons in a dense layer with ReLU activation\n",
    "* 512 neurons in a dense layer with ReLU activation\n",
    "* ```2*latent_dim``` output neurons in a dense layer with **no activation** for the mean and log variance parameters in the layer leading to the latent representation \n",
    "\n",
    "Define the following decoder architecture, which can be viewed as a reversal of the encoder steps:\n",
    "\n",
    "* Figure out what the input layer for the latent representation needs to look like. It should be a minor adjustment to what you have seen in [Notebook 6](../Notebooks/6-VAE.ipynb)\n",
    "* 512 neurons in a dense layer with ReLU activation\n",
    "* 7x7x16 neurons in a dense layer with ReLU activation\n",
    "* Reshape the output of the previous layer to ```target_shape=(7, 7, 16)```\n",
    "* 16 transposed 2d conv. filters of size $3 \\times 3$ with stride $2$, ```same``` padding, with ReLU activation \n",
    "* 6 transposed 2d conv. filters of size $3 \\times 3$ with stride $2$ , ```same``` padding, with ReLU activation\n",
    "* 1 transposed 2d conv. filters of size $3 \\times 3$ with stride $1$ , ```same``` padding, **no activation** for the output\n",
    "\n",
    "Additionally:\n",
    "* use ```keras.optimizers.Adam``` as the optimizer\n",
    "* use the same loss as seen in [Notebook 6](../Notebooks/6-VAE.ipynb)\n",
    "* store weights in ```output/Fashion-MNIST_condVAE_weights.h5```\n",
    "\n",
    "Hints: The following considerations might help you with the implementation.\n",
    "1. The input images in ```img_train``` and ```img_test``` are of shape ```[batch_size, 28, 28, 1]```. For the encoder input, adjust the labels ```y``` as a similar tensor of shape ```[batch_size, 28, 28, 1]``` where the 28x28 entries are just the label repeated in every entry.\n",
    "2. The latent code ```latent_z``` will be of shape ```[batch_size, latent_dim]```. For the decoder input, adjust the labels  ```y``` as a similar tensor of shape ```[batch_size, 1]```.\n",
    "\n",
    "You can use the following code for this:\n",
    "\n",
    "```Python\n",
    "# merge image and label\n",
    "x_shapes = tf.shape(x)\n",
    "y = tf.reshape(y, [self.batch_size, 1, 1, 1])\n",
    "x = tf.concat([x, y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], 1])], axis=3)\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```Python\n",
    "# merge noise and label\n",
    "y = tf.reshape(y, [self.batch_size, 1])\n",
    "z = tf.concat([z, y], axis=1)\n",
    "```\n",
    "\n",
    "3. When you get an error like <center><img src=\"src/A4_Potential_Error_Msg.png\" width=\"800\"/></center> execute the cell where you define your conditional VAE class and training functions etc. once more.\n",
    "\n",
    "You can use the following hyperparameters but also explore other choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3039a9-5b35-4667-b455-eaeddcfce480",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "latent_dim = 10\n",
    "\n",
    "num_train = img_train.shape[0]\n",
    "num_test = img_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5575ae12-f39e-48eb-86a8-c1745f3a5db9",
   "metadata": {},
   "source": [
    "### Exercise II.A.1\n",
    "\n",
    "Put your attempt at implementing the conditional VAE below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9fcf4b-714d-45ab-b583-33cf2dde8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.utils.set_random_seed(123)\n",
    "\n",
    "class condVAE(keras.Model):\n",
    "    # Please fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e569c899-f105-4f03-b4b3-a5df12ae5d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "072513f1-c400-4952-b691-74849efc5063",
   "metadata": {},
   "source": [
    "### Exercise II.A.2\n",
    "\n",
    "For the test set images, plot the reconstructions below.\n",
    "\n",
    "Hint: You will need to use something similar to \n",
    "\n",
    "```Python\n",
    "predictions = condVAE_model.sample(y=label_test[:batch_size], eps=latent_z)\n",
    "```\n",
    "\n",
    "for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaafd4f7-11ef-4d33-ae05-a0ef8f264c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac82540a-601c-4605-b003-3220695c0175",
   "metadata": {},
   "source": [
    "### Exercise II.A.3\n",
    "\n",
    "Plot the latent space below. I.e. plot the first two latent dimensions with the latent representation of\n",
    "the first batch of test data points below.\n",
    "\n",
    "Hint: You can use ```cmap='tab10'``` as a categorical colour map and\n",
    "\n",
    "```Python\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_yticklabels(fashion_labels.values())\n",
    "```\n",
    "\n",
    "to have the fashion item labels displayed in the colorbar (instead of integer values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b1f432-569b-4c5d-828d-d21840ff048a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5d35ef-fdee-4860-ba49-de470cfabac7",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Subpart II.B - Fashion VAE\n",
    "\n",
    "Implementing a conditional VAE is a challenging task! If you could not complete subpart II.A, \n",
    "you can implement the standard VAE in this subpart instead. Please leave your attempt on II.A in the cells above as they are!\n",
    "\n",
    "Recall the VAE implementation covered in [Notebook 6](../Notebooks/6-VAE.ipynb). You can follow closely what we have used there.\n",
    "\n",
    "Define the following encoder architecture:\n",
    "\n",
    "* 28 $ \\times $ 28 $ \\times $ 1 neurons in the input layer for the greyscale image\n",
    "* 6 conv. filters of size $3 \\times 3$ with stride $2$ with ReLU activation followed by a $2 \\times 2$ max pooling\n",
    "* 16 conv. filters of size $3 \\times 3$ with stride $2$ with ReLU activation followed by a $2 \\times 2$ max pooling\n",
    "* flatten these feature maps in a single vector \n",
    "* 1024 neurons in a dense layer with ReLU activation\n",
    "* 512 neurons in a dense layer with ReLU activation\n",
    "* ```2*latent_dim``` output neurons in a dense layer with **no activation** for the mean and log variance parameters in the layer leading to the latent representation \n",
    "\n",
    "Define the following decoder architecture, which can be viewed as a reversal of the encoder steps:\n",
    "\n",
    "* An input layer which expects ```input_shape=(latent_dim,)```\n",
    "* 512 neurons in a dense layer with ReLU activation\n",
    "* 7x7x16 neurons in a dense layer with ReLU activation\n",
    "* Reshape the output of the previous layer to ```target_shape=(7, 7, 16)```\n",
    "* 16 transposed 2d conv. filters of size $3 \\times 3$ with stride $2$, ```same``` padding, with ReLU activation \n",
    "* 6 transposed 2d conv. filters of size $3 \\times 3$ with stride $2$ , ```same``` padding, with ReLU activation\n",
    "* 1 transposed 2d conv. filters of size $3 \\times 3$ with stride $1$ , ```same``` padding, **no activation** for the output\n",
    "\n",
    "Additionally:\n",
    "* use ```keras.optimizers.Adam``` as the optimizer\n",
    "* use the same loss as seen in [Notebook 6](../Notebooks/6-VAE.ipynb)\n",
    "* store weights in ```output/Fashion-MNIST_VAE_weights.h5```\n",
    "\n",
    "Hints: When you get an error like <center><img src=\"src/A4_Potential_Error_Msg.png\" width=\"800\"/></center> execute the cell where you define your VAE class and training functions etc. once more.\n",
    "\n",
    "You can use the following hyperparameters but also explore other choices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3cb88b-341a-4aac-b28e-5e3fcb8a7064",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "latent_dim = 10\n",
    "\n",
    "num_train = img_train.shape[0]\n",
    "num_test = img_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc938c-a527-499a-a119-a49354399422",
   "metadata": {},
   "source": [
    "### Exercise II.B.1\n",
    "\n",
    "Put your implementation of the VAE below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4950aba5-2421-4aa0-8c8b-462875c82cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.utils.set_random_seed(123)\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    # Please fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab39f69-f1a1-4f21-8488-6d3b8550b794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2358042d-ff8c-49b6-a179-fd0a224d356a",
   "metadata": {},
   "source": [
    "### Exercise II.B.2\n",
    "\n",
    "For the test set images, plot the reconstructions below.\n",
    "\n",
    "Hint: You will need to use something similar to \n",
    "\n",
    "```Python\n",
    "predictions = VAE_model.sample(latent_z)\n",
    "```\n",
    "\n",
    "for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d88ba-4b75-4476-a70f-6e9598ace736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0784188f-be24-4cfb-afc0-a61a74512d83",
   "metadata": {},
   "source": [
    "### Exercise II.B.3\n",
    "\n",
    "Plot the latent space below. I.e. plot the first two latent dimensions with the latent representation of\n",
    "the first batch of test data points below.\n",
    "\n",
    "Hint: You can use ```cmap='tab10'``` as a categorical colour map and\n",
    "\n",
    "```Python\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_yticklabels(fashion_labels.values())\n",
    "```\n",
    "\n",
    "to have the fashion item labels displayed in the colorbar (instead of integer values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94643f0a-201b-466a-912b-4f53d5e74845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f134116-9454-4ef9-a2d5-21e0c0f290f9",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Part III: Exploring embeddings\n",
    "\n",
    "In Part II, you should have implemented a way to generate novel, \"artificial\" fashion\n",
    "items by sampling them in the latent space and decoding these samples into images. Your \n",
    "procedure could look something like this for subpart II.A\n",
    "\n",
    "```Python\n",
    "generated_images = condVAE_model.sample(y=label_to_generate, eps=latent_z)\n",
    "```\n",
    "\n",
    "or like this for subpart II.B\n",
    "\n",
    "```Python\n",
    "generated_images = vae_model.sample(eps=latent_z)\n",
    "```\n",
    "\n",
    "with shapes \n",
    "* ```label_to_generate.shape = (batch_size,)```\n",
    "* ```latent_z.shape = (batch_size, latent_dim)```\n",
    "* ```generated_images.shape = (batch_size, 28, 28, 1)```\n",
    "\n",
    "Make use of this sampling procedure in the following exercise.\n",
    "\n",
    "### Exercise III.1\n",
    "\n",
    "Generate random samples from the standard normal distribution \n",
    "with the correct shape for ```latent_z``` and create novel, artificial fashion items.\n",
    "\n",
    "If you successfully implemented the conditional VAE, create the label input similar to ```y=LABELID*np.ones(batch_size).astype(np.float32)```\n",
    "and choose ```LABELID``` as ```1``` (i.e. 'Trouser') and ```9``` (i.e. 'Ankle boot').\n",
    "You can also choose to condition the decoder to other labels.\n",
    "\n",
    "Plot your novel, artificial fashion items similar to your results in II.A.2 or II.B.2.\n",
    "You should expect some of them to look quite freaky as you mix different fashion items. :)\n",
    "\n",
    "Fill in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0886be-c7f3-4407-b770-3ab924c28346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e18658d-a983-4ba1-af3d-b7fb3b817f1b",
   "metadata": {},
   "source": [
    "### Bonus Exercise III.2\n",
    "\n",
    "#### This last part is not required to complete assignment 4!\n",
    "\n",
    "It is difficult to visualise more than three-dimensional latent spaces. \n",
    "For this reason, you have visualised only two latent dimensions in\n",
    "Exercises II.A.3 or II.B.3. \n",
    "\n",
    "However, there are some tools to generate two-dimensional visualisation of \n",
    "higher-dimensional data, such as **t-distributed Stochastic Neighbor Embedding (t-SNE)**.\n",
    "You can read up on it in the [scikit-learn documentation on t-SNE](https://scikit-learn.org/stable/modules/manifold.html#t-sne).\n",
    "\n",
    "You can explore its capabilities below if you want to.\n",
    "\n",
    "First, load the latent representation for a batch of the test data.\n",
    "You can copy part of your results used in Exercises II.A.3 or II.B.3 here.\n",
    "\n",
    "Fill in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4e4f1b-1afe-447d-9e4c-a5677e3017f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_z = # Please fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678f14f-936e-46a1-8d2d-ec2e0180a6ec",
   "metadata": {},
   "source": [
    "Let's assume the latent representation for the images with labels ```label_test[:batch_size]```\n",
    "is given by ```latent_z```. Then you can execute the following to display the t-SNE visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb7ad1-2df4-4217-88ba-3e5869ffe9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_visualisation = TSNE(n_components=2, \n",
    "                          learning_rate='auto',\n",
    "                          init='random', \n",
    "                          perplexity=3).fit_transform(latent_z)\n",
    "\n",
    "plt.scatter(tsne_visualisation[:,0], tsne_visualisation[:,1],  c=label_test[:batch_size], cmap='tab10')\n",
    "cbar = plt.colorbar()\n",
    "cbar.ax.set_yticklabels(fashion_labels.values())\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
