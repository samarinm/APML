{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d65f127-2699-4e81-abe1-12a1d3dab204",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Kernel Methods\n",
    "\n",
    "In contrast to the idea of dimensionality reduction and compression, another popular direction in Machine Learning \n",
    "considers the opposite approach of solving problems in a higher-dimensional space. In this notebook we present the \n",
    "general idea of **kernel methods** on the example of\n",
    "\n",
    "* Kernel ridge regression and\n",
    "* Gaussian Processes\n",
    "\n",
    "mostly focusing on simulated data to highlight their main benefits.\n",
    "\n",
    "Keywords: ```feature mapping```, ```RBF kernel```, ```sklearn.kernel_ridge.KernelRidge```, \n",
    " ```sklearn.gaussian_process.GaussianProcessRegressor```, ```sklearn.gaussian_process.kernels.RBF```\n",
    " ```sklearn.pipeline.make_pipeline```, ```sklearn.preprocessing.StandardScaler```\n",
    "\n",
    "***\n",
    "\n",
    "## Lifting Data into higher Dimensions\n",
    "\n",
    "All models considered in the previous notebooks can be regarded as essentially linear models. Although linear models have \n",
    "their advantages (in particular with respect to mathemical derivations and guarantees), they severly limit the kind of \n",
    "functions we can represent. A common approach therefore is to **map** the problem from the input space into a \n",
    "**higher-dimensional feature space** as illustrated in the next figure for a classification setting.\n",
    "\n",
    "<center><img src=\"images/Lifting.png\" alt=\"Lifting Data into Higher-Dimensions\" width=\"600\"/></center>\n",
    "\n",
    "The feature mapping $\\phi$ allows us to map data from a lower-dimensional input space into a higher-dimensional feature \n",
    "space. The key idea is that identifying the complicated decision boundary in input space might be challenging, but\n",
    "**in feature space a linear model suffices** to identify the planar decision boundary. \n",
    "\n",
    "More importantly, we can show that we do not need to ever evaluate $\\phi(x)$. We can rely on the **kernel trick**!\n",
    "This implies that we only need to evalute scalar products of feature mappings -- called kernels -- $\\kappa(x,x')=\\langle \\phi(x),\\phi(x')\\rangle$ for pairs of datapoints $x,x'$ in our dataset $X$.\n",
    "\n",
    "One such wildly used kernel is the \n",
    "**radial basis function (RBF) kernel**\n",
    "\n",
    "\\begin{equation}\n",
    "    \\kappa (x,x') = \\exp \\Big( - \\frac{1}{2\\sigma^2} \\Vert x - x' \\Vert_2^2 \\Big)\n",
    "\\end{equation}\n",
    "\n",
    "which corresponds to an infinite-dimensional feature space. In general, we can design task-dependent kernels which might\n",
    "extract features we deem relevant to our problem. Furthermore, we can \"kernelise\" standard linear approaches to \n",
    "become more powerful!\n",
    "\n",
    "***\n",
    "\n",
    "## Kernel Ridge Regression\n",
    "\n",
    "We revisit our standard regression techniques and see how the kernelised version extends what we have \n",
    "seen before. We formulated the linear regression model before as\n",
    "\n",
    "\\begin{equation}\n",
    "    Y = Xw + \\epsilon.\n",
    "\\end{equation}\n",
    "\n",
    "Let's simulate a sinusoidal dataset (with noise) of $N=200$ datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c6e94-f977-4dca-8643-304b64781e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "num_datapoints = 200\n",
    "X = np.random.rand(num_datapoints)\n",
    "X = np.sort(X).reshape(-1,1)\n",
    "\n",
    "Y = np.sin(4 * np.pi * X) + np.random.randn(num_datapoints, 1) * 0.1 + 5\n",
    "\n",
    "plt.scatter(X,Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ab355-a5ad-4cfd-8ac4-3144c15c19bc",
   "metadata": {},
   "source": [
    "As we have seen before, we will try to fit these observations with a\n",
    "polynomial with linear and ridge regression. \n",
    "\n",
    "In the following, we make use of an advanced ```scikit-learn``` tool.\n",
    "We build a **pipeline** (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline))\n",
    "which allows data processing steps to be executed with an estimator like  ```LinearRegression``` or ```Ridge```. \n",
    "In particular, we use the ```StandardScalar``` method (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)) which is a method to \n",
    "standardise our dataset, i.e. subtract the sample mean and divide by the sample\n",
    "standard deviation. With such a pipeline we can do some \"data curation\" before\n",
    "training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92525e-05b6-4434-ab6f-15b77c63219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_poly = np.hstack((X, X ** 2, X ** 3, X ** 4, X ** 5))\n",
    "\n",
    "linreg = make_pipeline(StandardScaler(), LinearRegression())\n",
    "linreg.fit(X_poly,Y)\n",
    "Y_OLS = linreg.predict(X_poly)\n",
    "Rsquared_lin = linreg.score(X_poly,Y)\n",
    "\n",
    "ridgereg = make_pipeline(StandardScaler(), Ridge(alpha=0.001))\n",
    "ridgereg.fit(X_poly,Y)\n",
    "Y_ridge = ridgereg.predict(X_poly)\n",
    "Rsquared_ridge = ridgereg.score(X_poly,Y)\n",
    "\n",
    "print(f\"Rsquared_lin = {Rsquared_lin:.2f}, Rsquared_ridge = {Rsquared_ridge:.2f}\")\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(8,3))\n",
    "\n",
    "axs[0].scatter(X[:, 0], Y, color='red')\n",
    "axs[0].plot(X[:, 0], Y_OLS, color='blue')\n",
    "\n",
    "axs[1].scatter(X[:, 0], Y, color='red')\n",
    "axs[1].plot(X[:, 0], Y_ridge, color='green')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7acaa9-cdf2-4bcb-8a72-ea34b01c11c6",
   "metadata": {},
   "source": [
    "### **Kernel Ridge Regression (KRR)** \n",
    "incorporates the idea of employing a feature \n",
    "mapping $\\phi$ and we rewrite the linear regression model as\n",
    "\n",
    "\\begin{equation}\n",
    "    Y = \\phi(X)w + \\epsilon = f(x) + \\epsilon.\n",
    "\\end{equation}\n",
    "\n",
    "Similar to the ridge regression result, the solution can be shown to be \n",
    "\n",
    "\\begin{equation}\n",
    "    w_\\text{KRR} = \\phi(X)^\\top \\left( \\phi(X)^\\top \\phi(X) + \\lambda \\mathbb{1}_{N} \\right)^{-1} Y.\n",
    "\\end{equation}\n",
    "\n",
    "and the learned function (without going into the mathematical details) is then given by \n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = K(x,X) \\left( K(X,X) + \\lambda \\mathbb{1}_{d_N} \\right)^{-1} Y\n",
    "\\end{equation}\n",
    "\n",
    "with $K$ being the kernel matrix $K_{nm} = \\kappa(x_n, x_m) = \\langle \\phi(x_n),\\phi(x_m)\\rangle$. \n",
    "\n",
    "Let's see how we can make use of for the above dataset and define the RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da239619-f073-43f0-92e9-93efaad7d8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x1, x2, sigma=1.0):\n",
    "    return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * sigma ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a922ab-ed5e-424a-9ef5-cb7055770e4c",
   "metadata": {},
   "source": [
    "As usually, we use a ```scikit-learn``` estimator: [KernelRidge](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20763e0-9dc6-49fd-ab23-a3549c96e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "KRR = make_pipeline(StandardScaler(), KernelRidge(kernel=rbf_kernel, alpha=0.01, gamma=0.1))\n",
    "KRR.fit(X, Y)\n",
    "Y_KRR = KRR.predict(X)\n",
    "\n",
    "Rsquared_KRR = KRR.score(X,Y)\n",
    "\n",
    "print(f\"Rsquared_lin = {Rsquared_lin:.2f}, Rsquared_ridge = {Rsquared_ridge:.2f}, Rsquared_KRR = {Rsquared_KRR:.2f}\")\n",
    "\n",
    "plt.scatter(X[:, 0], Y, color='red')\n",
    "plt.plot(X[:, 0], Y_KRR, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5976b9-78d9-4900-9a6e-ebb572b41aaf",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Gaussian Processes\n",
    "\n",
    "Kernel Ridge Regression has a close connection to **Gaussian Processes (GPs)** \n",
    "([Wikipedia](https://en.wikipedia.org/wiki/Gaussian_process) or [scikit-learn](https://scikit-learn.org/stable/modules/gaussian_process.html)). Informally, a GP \n",
    "extends the notion of random variables to random functions. In a regression setting with input \n",
    "matrix $X$, the GP prior distribution $p(f \\mid X) = \\mathcal{N} (0, K)$ allows us to model the \n",
    "regression function $f(X)= [f(x_1), ..., f(x_N)]^\\top$ as \n",
    "\n",
    "\\begin{equation}\n",
    "    \\big( f(x_1), ..., f(x_N)\\big) \\sim \\mathcal{N} \\left( 0, K \\right)\n",
    "\\end{equation}\n",
    "\n",
    "with sample covariance matrix $K=\\text{cov}(X,X)$. Let us consider a GP regression model with noisy \n",
    "responses $Y$ as before, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "    Y = f(x) + \\epsilon.\n",
    "\\end{equation}\n",
    "\n",
    "Leaving the mathematical derivation aside, in order to make a prediction for function $f(x)$ \n",
    "with new (test) data $x$, we can show that the posterior predictive distribution \n",
    "([Wikipedia](https://en.wikipedia.org/wiki/Posterior_predictive_distribution)) has the form\n",
    "\n",
    "\\begin{align}\n",
    "    p(f \\mid x, X, Y) &= \\mathcal{N} ( \\mu(x), \\Sigma(x,X) )  \\\\\n",
    "    \\mu(x) &= K(x,X) \\big( K(X,X) + \\lambda\\mathbb{1}_N \\big)^{-1}  Y \\\\\n",
    "    \\Sigma(x,X) &= K(x,x) - K(x,X)^\\top  \\left( K(X,X) + \\lambda \\mathbb{1}_N \\right)^{-1} K(x,X)\n",
    "\\end{align}\n",
    "\n",
    "Note that what we have written earlier for $f(x)$ in KRR **corresponds exactly** to $\\mu(x)$ in\n",
    "GP regression, i.e. there is a close connection between KRR and GP regression (see e.g. [this comparison](https://scikit-learn.org/0.24/auto_examples/gaussian_process/plot_compare_gpr_krr.html)). This motivates that\n",
    "the **covariance matrix $K$ is a kernel**, too! Compared to KRR, GPs can be more flexible (as we can sample\n",
    "functions from the posterior) and we can make use of the GP formalism for training. Also, GPs enable us to quantify the uncertainty \n",
    "of a prediction due to $\\Sigma(x,X)$. \n",
    "\n",
    "Let's study this in a simplified example with $N=4$ datapoints. As always, we can use\n",
    "```scikit-learn``` methods for the [GaussianProcessRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor) and different kernels.\n",
    "We employ the [RBF](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html#sklearn.gaussian_process.kernels.RBF) kernel as well as a \n",
    "[ConstantKernel](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ConstantKernel.html), i.e.\n",
    "$\\kappa (x,x') = \\text{const}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b0b20-64a2-465e-be4b-a396522ff8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "num_datapoints = 4\n",
    "\n",
    "X = np.random.rand(num_datapoints, 1)*2\n",
    "Y = np.sin(2 * np.pi * X) + np.random.randn(num_datapoints, 1) * 0.1 + 5\n",
    "\n",
    "kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(0.001, 1000)) \n",
    "kernel = kernel * RBF(length_scale=0.5, length_scale_bounds=(0.01, 100))\n",
    "\n",
    "GPreg = GaussianProcessRegressor(kernel=kernel, alpha=0.1, n_restarts_optimizer=10)\n",
    "GPreg.fit(X, Y)\n",
    "\n",
    "X_pred = np.linspace(-2.0, 2.0, 100).reshape(-1, 1)\n",
    "Y_GPreg, stddev_GPreg = GPreg.predict(X_pred, return_std=True)\n",
    "\n",
    "X_pred = X_pred.reshape(-1)\n",
    "Y_GPreg = Y_GPreg.reshape(-1)\n",
    "\n",
    "plt.fill_between(X_pred, Y_GPreg - stddev_GPreg,  Y_GPreg + stddev_GPreg, alpha=0.3, color='grey')\n",
    "plt.plot(X_pred, Y_GPreg, color='blue', label='GP mean prediction')\n",
    "plt.scatter(X, Y, color='red', label='Observations')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e205c-9c23-4948-9d39-fa7f7d0d115d",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "## Exercise Section\n",
    "\n",
    "For the exercises, we consider again the red wine dataset but this time train a \n",
    "(1.) kernel ridge regressor and (2.) Gaussian Process regressor. As in notebook 2, \n",
    "we want to predict the ```quality``` values on the test set ```feat_test``` and \n",
    "compare the mean squared errors (MSEs) of KRR and GP regression to the previous \n",
    "results for ridge regression (0.42) and Random Forest regression (0.34).\n",
    "\n",
    "In order, to prepare the data and kernel, load the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1654fac-85ff-4d60-a737-0544a9200b86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "ex_kernel = RBF(length_scale=5, length_scale_bounds=(0.01, 100))\n",
    "\n",
    "wine_data = pd.read_csv('data/winequality-red.csv', sep=';')\n",
    "\n",
    "ex_target = wine_data['quality']\n",
    "ex_features = wine_data.drop(['quality'], axis=1)\n",
    "\n",
    "feat_train, feat_test, target_train, target_test = train_test_split(\n",
    "    ex_features, ex_target, test_size = 0.3, random_state=123)\n",
    "\n",
    "target_test = target_test.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a86e9c-f988-4583-be5e-7cb3e7c7a7f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feat_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b00a71-0c92-4673-b213-b26a79579a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d769e1-7b07-4664-b800-1753b2cf177e",
   "metadata": {},
   "source": [
    "(1.) Implement kernel ridge regression as we have seen it in this notebook. \n",
    "Please provide the predictions for the test set ```feat_test``` in ```ex_pred_KRR```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f19c2-6009-434b-a1a4-7c45e4ae2831",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b18cf-3b04-4947-9dad-be19a9e546fc",
   "metadata": {},
   "source": [
    "We use ```ex_pred_KRR``` in the following cell to provide the results. \n",
    "You can just execute the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc3281b-4d72-47eb-9784-aed4b29bb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test['KRR_predicted_quality'] = np.around(ex_pred_KRR, decimals=2)\n",
    "target_test['KRR_absolut_deviation'] = abs(target_test['quality'] - target_test['KRR_predicted_quality'])\n",
    "print(target_test)\n",
    "\n",
    "KRR_pred_MSE = (target_test['KRR_absolut_deviation']**2).mean()\n",
    "print(f\"\\nMean squared error of KernelRidge: {KRR_pred_MSE:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9efece-59e5-4163-bf55-8f21cad4acfe",
   "metadata": {},
   "source": [
    "(2.) Implement GP regression as we have seen it in this notebook. \n",
    "Please provide the predictions for the test set ```feat_test``` in ```ex_pred_GPreg```.\n",
    "\n",
    "**For Noto users**: You might want to set ```n_restarts_optimizer=1```(i.e. only one), as otherwise \n",
    "the computation might take too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d61034-fa19-42e5-ba37-9d1e4516e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af177018-6e66-467b-9e8d-690faf8625ab",
   "metadata": {},
   "source": [
    "We use ```ex_pred_GPreg``` in the following cell to provide the results. \n",
    "You can just execute the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5566509a-4761-4c7c-9308-96ebe323aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test['GPreg_predicted_quality'] = np.around(ex_pred_GPreg, decimals=2)\n",
    "target_test['GPreg_absolut_deviation'] = abs(target_test['quality'] - target_test['GPreg_predicted_quality'])\n",
    "print(target_test)\n",
    "\n",
    "GPreg_pred_MSE = (target_test['GPreg_absolut_deviation']**2).mean()\n",
    "\n",
    "print(f\"\\nMean squared error of GaussianProcessRegressor: {GPreg_pred_MSE:.2f}\")\n",
    "print(f\"Mean squared error of KernelRidge: {KRR_pred_MSE:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
