{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ccba99-91f7-4a17-b25d-d9ea83ad3314",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Dimensionality Reduction and Clustering\n",
    "\n",
    "In the previous notebook, we could rely on labels which supervised the learning algorithm.\n",
    "Next, we consider the unsupervised setting and knowledge discovery. We focus on the idea\n",
    "of identifying a subset of relevant dimensions which capture the most variation in the data. \n",
    "This is closely related to the idea of compression.\n",
    "\n",
    "In this notebook, we cover\n",
    "\n",
    "* Principal Component Analysis (PCA) for dimensionality reduction (with a finance application)\n",
    "* k-means clustering \n",
    "* Gaussian Mixture Models (GMMs)\n",
    "\n",
    "and apply these techniques to simulated data and two dataset examples (digit recognition and\n",
    "financial data).\n",
    "\n",
    "Keywords:  ```MNIST```, ```np.linalg.eig```, ```sklearn.decomposition.PCA```, ```sklearn.cluster.KMeans```, \n",
    " ```sklearn.mixture.GaussianMixture```\n",
    "\n",
    "***\n",
    "\n",
    "## Dimensionality Reduction \n",
    "\n",
    "The topic of dimensionality reduction plays a vital role in Machine Learning. Typically, our\n",
    "datasets collect a great number of predictors or covariates or consist of images with hundreds of \n",
    "pixels. Getting an idea of the input space and relevant features is challenging in these high-dimensional\n",
    "problems. Three main use cases include:\n",
    "\n",
    "1. **Identifying relevant features / dimensions** in the input space which as much as possible preserves the variance in the data. In the following simplified example, the black arrows indicate the dimensions of highest variance: <center><img src=\"images/PCA_example.png\" alt=\"PCA Example\" width=\"300\"/></center> The longer arrow can be viewed as the **main direction / dimension of variance** which accounts for about 90% of total variance and thus as the \"more relevant\" feature.\n",
    "\n",
    "2. **Data compression**: In many applications we can reduce the number of dimensions to capture the relevant information. In the previous image, we might choose the longer arrow as a compressed version of our data. Another compression example is shown in the next image: <center><img src=\"images/Compression.png\" alt=\"Image compression with Fast Fourier Transform\" width=\"700\"/></center> Here, we encoded an image of Maxim's cat (with an approach called *Fast Fourier Transformation*) into a much lower dimensional compressed image which only requires 9% of pixels compared to the original image. Decoding this compressed image reconstructs the orginial image almost perfectly.\n",
    "\n",
    "3. **Visualising** high-dimensional data is generally difficult for dimensions larger than 3. Thus, dimensionality reduction is an important tool for visualisation and identifying patterns.\n",
    "\n",
    "***\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) ([Wikipeda](https://en.wikipedia.org/wiki/Principal_component_analysis)) is such a standard technique used for the above purposes and is of great relevance in a great variety of different disciplines. \n",
    "\n",
    "Let us generate simple 2-dimensional dataset to identify the main dimensions of variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52392217-08e9-4d7b-b10d-06f608f003ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "num_datapoints = 1000\n",
    "\n",
    "mu_generation = [2,4]\n",
    "covariance_matrix_generation = np.array([[1,    0.75],\n",
    "                                         [0.75, 1,  ]\n",
    "                                        ])\n",
    "\n",
    "gauss_2d_rotated = np.random.multivariate_normal(mean=mu_generation, \n",
    "                                                 cov=covariance_matrix_generation, \n",
    "                                                 size=num_datapoints)\n",
    "\n",
    "plt.scatter(gauss_2d_rotated[:,0], gauss_2d_rotated[:,1], c=gauss_2d_rotated[:,0], cmap='turbo')\n",
    "\n",
    "plt.ylim(0.5,8.5)\n",
    "plt.xlim(-1.5,6.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f213fd-c53a-4eaa-96ca-5a623b456b52",
   "metadata": {},
   "source": [
    "The fundamental idea of PCA is to perform an eigendecomposition on the data covariance matrix $\\text{cov}(X,X)$.\n",
    "If our dataset $X$ is **centered**, i.e. the mean of all datapoints $N$ is 0, we can write the (sample) covariance matrix as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{cov}(X,X) = \\frac{1}{N-1} XX^\\top.\n",
    "\\end{equation}\n",
    "\n",
    "The corresponding eigenvectors to the largest eigenvalues of this matrix provide the\n",
    "main axes of variation (principle components) in our dataset $X$.\n",
    "\n",
    "Let's calculate these for our example and start by centering the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c1962-3a2a-4208-bba7-a871d53d44aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_mean = np.mean(gauss_2d_rotated, axis=0)\n",
    "data_centered = gauss_2d_rotated - data_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c69b9a-4334-451f-95e6-07dbd8961bac",
   "metadata": {},
   "source": [
    "Next, calculate the eigenvalues and eigenvectors using ```np.linalg.eig``` and sort them in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156656b-d730-4cbc-a085-e753501a7e77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cov_matrix = np.cov(data_centered, rowvar=False)\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af3e3d0-d8cd-4747-a376-34231200648c",
   "metadata": {},
   "source": [
    "Let's compare the calculated covariance and the covariance matrix we used to generate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8100bc6e-4e68-46e5-a584-142c28bb958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Calculated covariance:\\n{cov_matrix}\\n\\nCovariance used for generation:\\n{covariance_matrix_generation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22326bd-48f0-4383-b411-af093d005a1b",
   "metadata": {},
   "source": [
    "Choose the two eigenvectors with the highest eigenvalues and project the data onto these vectors.\n",
    "These are our \"new\" x- and y-axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8f5bb-0b80-4b04-8a24-4743c0474b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_pca_transform = np.dot(data_centered, eigenvectors[:, :2])\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10,3))\n",
    "\n",
    "axs[0].scatter(data_pca_transform[:, 0], data_pca_transform[:, 1], c=gauss_2d_rotated[:,0], cmap='turbo')\n",
    "\n",
    "axs[0].set_xlabel('First Principal Component')\n",
    "axs[0].set_ylabel('Second Principal Component')\n",
    "axs[0].set_xlim(-4,5.5)\n",
    "axs[0].set_ylim(-4,4)\n",
    "\n",
    "axs[1].scatter(gauss_2d_rotated[:,0], gauss_2d_rotated[:,1], c=gauss_2d_rotated[:,0], cmap='turbo')\n",
    "\n",
    "axs[1].set_xlabel('Original x-axis')\n",
    "axs[1].set_ylabel('Original y-axis')\n",
    "axs[1].set_ylim(0.5,8.5)\n",
    "axs[1].set_xlim(-1.5,6.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b463a-316d-49cd-b14e-78f092a5bc1a",
   "metadata": {},
   "source": [
    "#### Note \n",
    "that in this 2d example, the **projection on the two eigenvectors corresponds to a rotation** of our dataset.\n",
    "This will be different, if the original dataset has more dimensions.\n",
    "\n",
    "### As before, we can just use ```scikit-learn``` \n",
    "and its functionality with the function [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbaa199-a8d6-4919-9892-069013039147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "num_eigvectors = 2\n",
    "\n",
    "data_pca = PCA(n_components=num_eigvectors)\n",
    "data_pca.fit(gauss_2d_rotated)\n",
    "\n",
    "data_pca_transform = data_pca.transform(gauss_2d_rotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b876f6e-ee28-49af-a87f-11c56c4d10de",
   "metadata": {},
   "source": [
    "We can even check the explained variance ratio of the first two principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d453123-1823-412e-bd0d-ac07feade9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50892f6-0dbc-414a-89a9-8102294adb6d",
   "metadata": {},
   "source": [
    "### MNIST Example \n",
    "As a first, more interesting example, we examine the MNIST dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6d4b4-69c6-43d9-90f4-7959b1ade057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mnist_data = np.load('data/mnist_data_5k.npy', allow_pickle=True)\n",
    "mnist_targets = np.load('data/mnist_labels_5k.npy', allow_pickle=True)\n",
    "\n",
    "fig, axs = plt.subplots(1,5)\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(mnist_data[i].reshape(28,28))\n",
    "    \n",
    "    ax.set_title(f\"Label: {mnist_targets[i]}\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa70f3-2611-45d4-b197-2301c513caf5",
   "metadata": {},
   "source": [
    "Let's see what the projection on the first two eigenvectors looks like in MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd426b-15f1-45e0-87ea-455797a97599",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca = PCA(n_components=2)\n",
    "data_pca.fit(mnist_data)\n",
    "data_pca_transform_mnist = data_pca.transform(mnist_data)\n",
    "\n",
    "plt.scatter(data_pca_transform_mnist[:, 0], data_pca_transform_mnist[:, 1], c=mnist_targets, cmap='tab10')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3f125-5e38-400f-b172-ab90c09c9195",
   "metadata": {},
   "source": [
    "The result shows that **similar numbers are clustered** and thus that samples from the **same class share similar features**.\n",
    "\n",
    "### Finance Example \n",
    "Next, we consider an application in finance in which PCA can help to:\n",
    "\n",
    "1. Identify clusters of similar assets.\n",
    "2. Build predictive models where principal components can be used as new variables to predict some quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38250455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.precision', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebfe311-bfdf-4124-b6fa-fca0ec8435ad",
   "metadata": {},
   "source": [
    "In the first analysis, we consider a portfolio of companies in four industries: \n",
    "\n",
    "* automotive ('MBG.DE', 'BMW.DE')\n",
    "* airlines ('LHA.DE', 'AF.PA') \n",
    "* pharmaceuticals ('BAYN.DE', 'NVS', 'RO.SW')\n",
    "* tech ('AAPL', 'AMZN', 'GOOG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9bfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio = ['MBG.DE', 'BMW.DE', 'LHA.DE', 'AF.PA', \n",
    "             'BAYN.DE', 'NVS', 'RO.SW', 'AAPL', 'AMZN', 'GOOG']\n",
    "\n",
    "toy_df = pd.read_csv('data/finance_toy_data.csv', index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07876fc7-9990-4e36-92dc-192a5080e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77149a5-031c-46eb-80a8-a2ab66b0a4f0",
   "metadata": {},
   "source": [
    "We compute the normalised returns as well as the mean return and perform the PCA as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e40f59-0fd9-4c1f-ae2c-c096cb3d0c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_returns = toy_df.pct_change().dropna().T\n",
    "\n",
    "mean_return = norm_returns.mean(axis=1)\n",
    "\n",
    "num_eigvectors = 2\n",
    "\n",
    "data_pca = PCA(n_components=num_eigvectors)\n",
    "data_pca.fit(norm_returns)\n",
    "data_pca_transform = data_pca.transform(norm_returns)\n",
    "\n",
    "print(data_pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0d076-f5d0-4f58-99d5-8e36c6084068",
   "metadata": {},
   "source": [
    "Plot the first two principal components and label the individual datapoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b5d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_pca_transform[:, 0], data_pca_transform[:, 1], \n",
    "            c=mean_return.to_numpy()*100, cmap='viridis')\n",
    "\n",
    "for i, label in enumerate(portfolio):\n",
    "    plt.annotate(label, (data_pca_transform[i, 0], data_pca_transform[i, 1]), textcoords=\"offset points\", \n",
    "                 xytext=(-10, 7), ha='center', fontsize=8)\n",
    "\n",
    "plt.xlim(-0.55,0.8)\n",
    "plt.ylim(-0.4,0.5)\n",
    "\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcaefde-25ab-4aa0-8eb1-b286bd7a40e4",
   "metadata": {},
   "source": [
    "#### Interpretation of our results:\n",
    "\n",
    "* Assets with similar characteristics, such as tech companies (bottom-left corner) or airlines (bottom-right corner), tend to form clusters. \n",
    "\n",
    "* The first principal component can be interpreted as the **asset return axis**, while the second principal component represents the **volatility or variance of returns**.\n",
    "\n",
    "* For instance, **Apple (AAPL)** had high average returns, but also high volatility. On the other hand, **Lufthansa (LHA.DE)** yields negative gains but facing also high volatility (due to a sharp price drop during the covid crisis). This is why, AAPL and LHA.DE are closely positioned on the Y-axis but substantially deviate on the X-axis. \n",
    "\n",
    "* Companies in the upper-mid section, such as **BMW**, provided acceptable gains, and as a relatively conservative company, its volatility tends to be low. Hence, the scatter is centered on the X-axis and is at the upper part of the Y-axis.\n",
    "\n",
    "We extend our analysis to a high amount of assets. For this, we load data of the **DAX** which is a stock index that includes the 40 biggest German companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio = ['ADS.DE', 'ALV.DE', 'BAS.DE', 'BAYN.DE', 'BEI.DE', \n",
    "             'BMW.DE', 'CON.DE', 'DB1.DE', 'DBK.DE', 'DHER.DE', \n",
    "             'DPW.DE', 'DTE.DE', 'DWNI.DE', 'EOAN.DE', 'FME.DE', \n",
    "             'FRE.DE', 'HEI.DE', 'HEN3.DE', 'IFX.DE', 'LHA.DE', \n",
    "             'MRK.DE', 'MUV2.DE', 'RWE.DE', 'SAP.DE', 'SIE.DE', \n",
    "             'TKA.DE', 'VOW3.DE']\n",
    "\n",
    "dax_df = pd.read_csv('data/finance_dax_data.csv', index_col='Date')\n",
    "\n",
    "norm_returns = dax_df.pct_change().dropna().T\n",
    "\n",
    "mean_return = norm_returns.mean(axis=1)\n",
    "\n",
    "data_pca = PCA(n_components=2)\n",
    "data_pca.fit(norm_returns)\n",
    "data_pca_transform = data_pca.transform(norm_returns)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.scatter(data_pca_transform[:, 0], data_pca_transform[:, 1], \n",
    "            c=mean_return.to_numpy()*100, cmap='viridis')\n",
    "\n",
    "for i, label in enumerate(portfolio):\n",
    "    \n",
    "    plt.annotate(label, (data_pca_transform[i, 0], data_pca_transform[i, 1]), textcoords=\"offset points\", \n",
    "                 xytext=(0, 5), ha='center', fontsize=6)\n",
    "\n",
    "plt.xlim(-0.45,0.7)\n",
    "plt.ylim(-0.4,0.65)\n",
    "\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90f18ed-24aa-4221-bd8d-a93c5c6c9ea4",
   "metadata": {},
   "source": [
    "The DAX example shows that **two dimensions are insuffiecient to explain the data well**, as can be seen in the explained variance ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028267a-edd8-44d7-bb04-3066b81d9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Explained variance ratio:', data_pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8515136-44ed-40fd-b26e-bbc278f5de0d",
   "metadata": {},
   "source": [
    "This indicates that there are more important hidden features. Still, we can at least **detect outliers** \n",
    "in order to identify excellent or bad performing assets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d549ea-31b5-4375-bd9f-8661280079c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "## Clustering\n",
    "\n",
    "Another important approach to identifying patterns in an unsupervised fashion is clustering. In the previous examples,\n",
    "we have already seen instances where datapoints might form clusters indicating similar features.\n",
    "\n",
    "Two popular approaches to clustering are **k-means** and **Gaussian Mixture Models** which we consider in the following.\n",
    "\n",
    "#### k-means\n",
    "\n",
    "The underlying idea of k-means ([Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering)) is assign datapoints to $k$ cluster means / centroids in the dataset. The following GIF illustrates this for $k=3$ centroids:\n",
    "\n",
    "<center><img src=\"images/K-means_convergence.gif\" alt=\"k-means\" width=\"400\"/> <br> GIF source: <a href=\"https://en.wikipedia.org/wiki/K-means_clustering#/media/File:K-means_convergence.gif\">Wikipedia</a> <br> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333ea939-6c01-4327-a4e9-057408f7b626",
   "metadata": {
    "tags": []
   },
   "source": [
    "More formally, we consider a set of $N$ datapoints $X=(x_1, ..., x_N)$ which we try to group into \n",
    "$k$ groups forming sets $S_1, S_2, ..., S_k$. The objective function is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{argmin}_{S} \\sum_{i=1}^{k} \\sum_{x\\in S_i} \\Vert x - \\mu_i \\Vert ^2\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu_i = \\frac{1}{\\vert S_i \\vert} \\sum_{x\\in S_i} x$ is the mean of the datapoints in \n",
    "cluster $S_i$.\n",
    "\n",
    "The algorithm **starts** with placing the cluster means / centroids $\\mu_i$ in a random position \n",
    "(see GIF above at Iteration #0). Datapoints are assigned to the closest mean $\\mu_i$, i.e. to \n",
    "cluster $S_i$, and a **new cluster mean** is calculated. The algorithm **iterates** this step several \n",
    "times until the cluster assignments remain unchanged.\n",
    "\n",
    "#### Note\n",
    "that in an unsupervised setting, **we generally do not know the number of clusters $k$**! Thus, \n",
    "$k$ is a *hyper-parameter* we need to choose!\n",
    "\n",
    "Let's create a clustered dataset with two clusters of 100 datapoints each, i.e. $N=200$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d26ba-9f3f-4d36-b2a3-7596b36776a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "num_datapoints = 100\n",
    "\n",
    "cluster_1 = np.random.normal( 2, 1, size=(num_datapoints, 2))\n",
    "cluster_2 = np.random.normal(-2, 1, size=(num_datapoints, 2))\n",
    "\n",
    "cluster_data = np.concatenate([cluster_1, cluster_2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0a4ba-59dd-4026-843b-1a9e98d6ce06",
   "metadata": {},
   "source": [
    "We make use of the ```scikit-learn``` implementation of [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and choose $k=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f73b2-d09a-4a14-8046-01e339f40495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 2\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=123)\n",
    "kmeans.fit(cluster_data)\n",
    "\n",
    "cluster_prediction = kmeans.predict(cluster_data)\n",
    "\n",
    "plt.scatter(cluster_data[:,0], cluster_data[:,1], c=cluster_prediction)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be15a926-0f0a-4342-a16e-e8c360374c8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gaussian Mixture Model \n",
    "\n",
    "A Gaussian Mixture Model (GMM) ([Wikipedia](https://en.wikipedia.org/wiki/Mixture_model)) can be \n",
    "viewed as a (probabilistic) relaxation of k-means. Instead of \"hard\" cluster assignments, we \n",
    "consider \"soft\" assignments by means of probabilities. \n",
    "\n",
    "An GMM assumes a mixture of several Gaussian distributions as the underlying distribution for \n",
    "the dataset and observed clustering (which can be a strong assumption, in practice). Each Gaussian \n",
    "distribution $\\mathcal{N}(\\mu_i, \\Sigma_i)$ forms a cluster and is characterized by mean $\\mu_i$ \n",
    "and covariance matrix $\\Sigma_i$. The GMM (prior) distribution is then given by the mixture model\n",
    "\n",
    "\\begin{equation}\n",
    "    p(\\mu,\\Sigma) = \\sum_{i=1}^k w_i \\mathcal{N}(\\mu_i, \\Sigma_i)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu = (\\mu_1, ..., \\mu_k)$, $\\Sigma = (\\Sigma_1, ..., \\Sigma_k)$, and $w=(w_1,...,w_k)$ is \n",
    "the weighting of the different mixture components.\n",
    "\n",
    "As before, we start by choosing a number of $k$ clusters and setting the parameters $\\mu,\\Sigma$ to some\n",
    "initial values. The cluster assignment and parameters are then updated iteratively with the \n",
    "**expectation-maximisation (EM) algorithm** until cluster assignments, i.e. the parameters $\\mu,\\Sigma$, \n",
    "do not change any longer.\n",
    "\n",
    "Let's consider the same dataset as for k-means and make use of the ```scikit-learn``` \n",
    "[GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915987f-1e58-4c15-8aa1-f9d508e189fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "k = 2\n",
    "\n",
    "GMM = GaussianMixture(n_components=k)\n",
    "GMM.fit(cluster_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d08752-e469-47d8-a043-f9c303c90316",
   "metadata": {},
   "source": [
    "In order to provide the probabilities, we need to obtain a grid of datapoints\n",
    "which can do with ```numpy.meshgrid```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e254e30-cb98-4fd1-be41-3050861e6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = cluster_data[:, 0].min() - 1\n",
    "x_max = cluster_data[:, 0].max() + 1\n",
    "\n",
    "y_min = cluster_data[:, 1].min() - 1\n",
    "y_max = cluster_data[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), \n",
    "                     np.linspace(y_min, y_max, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e37a93-8d1c-4362-a307-1e207cf98892",
   "metadata": {},
   "source": [
    "Finally, we can obtain the probability predictions with ```score_samples``` and plot the results.\n",
    "Note that this method provides the log-likelihood, i.e. we need to exponentiate \n",
    "these outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c647b-ec25-4fb5-8b25-2fd79bbd62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "probab = GMM.score_samples(np.c_[xx.ravel(), yy.ravel()])\n",
    "probab = np.exp(probab)\n",
    "probab = probab.reshape(xx.shape)\n",
    "\n",
    "plt.scatter(cluster_data[:, 0], cluster_data[:, 1], c='black')\n",
    "plt.contourf(xx, yy, probab, alpha=0.6, cmap='BuPu')\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8ed11-146b-41b5-835e-fcd11047e030",
   "metadata": {},
   "source": [
    "#### Note\n",
    "that ```np.c_[A,B]``` concatenates NumPy arrays ```A``` and ```B```\n",
    "along the second axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205892c-6819-4597-ae9a-45dbdadc7bb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "## Exercise Section\n",
    "\n",
    "In the next two exercises, we use k-means and a Gaussian Mixture Model on the projection \n",
    "of the MNIST data on its two first principal components.\n",
    "\n",
    "(1.) Firstly, to simplify the clustering task, we will only consider digits ```0```, ```3```, \n",
    "```6```, and ```9```. For this make us of ```%3``` to identify all of these labels in\n",
    "```mnist_targets``` and select only the respective elements in ```data_pca_transform_mnist```\n",
    "and ```mnist_targets``` (defined above). Provide the filtered datasets ```ex_data``` and the corresponding\n",
    "labels in ```ex_targets``` in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dba2eb2-098e-48d2-8246-32a7eb941786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ex_data = # fill in\n",
    "ex_targets = # fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9355e42-f620-46bc-b6be-d6879f675f3d",
   "metadata": {},
   "source": [
    "If you were successful, you can plot the filtered dataset below,\n",
    "which should look like the following plot:\n",
    "\n",
    "<center><img src=\"images/MNIST_filtered.png\" alt=\"MNIST Filtered\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2b080-5f33-405a-81d5-bc72df8e1279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(ex_data[:, 0], ex_data[:, 1], c=ex_targets, cmap='tab10')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dcbdbb-effb-4915-9d8a-3b3d2171e3dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "(2.) Implement a k-means approach for clustering ```ex_data``` and provide the \n",
    "k-means object as ```kmeans``` and cluster predictions in ```cluster_prediction```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbbdf0e-61c6-4683-8b6b-41d5c1ccbdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535fba6-b631-4683-8433-cb8e34f1ecc4",
   "metadata": {},
   "source": [
    "Use ```cluster_prediction``` and the above defined ```kmeans``` to plot \n",
    "the clusters with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8a444-7d1a-4ff4-aadc-47cd672002f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ex_data[:,0], ex_data[:,1], c=cluster_prediction)\n",
    "plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], marker='X', c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd551c-1279-4ec5-b841-1f7df94c2a86",
   "metadata": {},
   "source": [
    "(3.) Implement an Gaussian Mixture Model approach for clustering ```ex_data```. Provide the \n",
    "Gaussian Mixture Model as object ```GMM```, the grid coordinates ```xx``` and ```yy``` (as seen before)\n",
    "and the predictions in ```probab```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b2c0a-e07f-4e24-a986-3c045a713d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ced598-7c43-48c1-86ee-9b777710cf0e",
   "metadata": {},
   "source": [
    "If you have defined ```GMM```, ```xx```, ```yy```, and ```probab``` in the cell,\n",
    "the next cell will allow you to plot your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41a30db-8835-44d5-8a28-44553c166ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ex_data[:, 0], ex_data[:, 1], c='black')\n",
    "plt.contourf(xx, yy, probab, alpha=0.6, cmap='BuPu')\n",
    "plt.scatter(GMM.means_[:,0],GMM.means_[:,1], marker='X', c='orange')\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
