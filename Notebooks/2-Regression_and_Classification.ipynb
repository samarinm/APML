{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb16d01-345b-4468-b011-ca7f2b64a894",
   "metadata": {},
   "source": [
    "# 2. Regression and Classification\n",
    "\n",
    "Regression and classficiation are two fundamental tasks of supervised Machine Learning where labels\n",
    "allows us to guide the learning. This notebook reviews more standard approaches to regression and classification.\n",
    "\n",
    "We provide the fundamental ideas behind\n",
    "\n",
    "* linear regression and ridge regression\n",
    "* logistic regression for classification\n",
    "* decision trees and Random Forests\n",
    "\n",
    "and apply these techniques to simulated data and a dataset example (wine quality assessment).\n",
    "\n",
    "Keywords: ```OLS```, ```MSE```, ```Overfittng```, ```Regularisation```, ```np.linalg.inv```, ```sklearn.linear_model.LinearRegression```, ```sklearn.linear_model.Ridge```, ```sklearn.tree.DecisionTreeClassifier```, ```sklearn.linear_model.LogisticRegression```, ```sklearn.ensemble.RandomForestClassifier```, ```sklearn.model_selection.train_test_split```\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf62adf-14ed-485b-bf72-2e593373fc13",
   "metadata": {},
   "source": [
    "#### We start by fixing a random seed\n",
    "which controls the generation of (pseudo) random number sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b810c4-f086-431c-bdc0-2e0cd4017ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae7437-d3ce-446e-a01d-9b2305ab9e2e",
   "metadata": {},
   "source": [
    "#### Note \n",
    "\n",
    "that this enables reproducibility of our results even in the presence of \"randomness\".\n",
    "For example, the first three runs of the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ca7838-717f-4010-8c4b-6a486d945b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.random(size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e0fa3-0d27-49d7-9a07-ebd114321483",
   "metadata": {},
   "source": [
    "will always produce\n",
    "\n",
    "1. ```array([0.69646919, 0.28613933, 0.22685145, 0.55131477])```\n",
    "\n",
    "2. ```array([0.71946897, 0.42310646, 0.9807642 , 0.68482974])```\n",
    "\n",
    "3. ```array([0.4809319 , 0.39211752, 0.34317802, 0.72904971])```\n",
    "\n",
    "***\n",
    "\n",
    "## Standard Linear Regression and Ordinary Least Squares\n",
    "\n",
    "We assume a linear relationship between true **response** $Y_\\text{groundtruth} = Xw$ and **predictors / covariates** $X$. \n",
    "However, usually **our observation is not perfect**, i.e. there is some noise additional $\\epsilon$. \n",
    "\n",
    "A simple linear regression model is then\n",
    "\n",
    "\\begin{equation}\n",
    "    Y = Xw + \\epsilon.\n",
    "\\end{equation}\n",
    "\n",
    "Let us simulate $N=10$ datapoints with Gaussian noise $\\epsilon \\sim \\mathcal{N}(\\mu=0,\\sigma=20)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51649dfc-9b39-4931-9bcb-466056165397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "num_datapoints = 10\n",
    "\n",
    "X = np.random.random(size=num_datapoints)*50\n",
    "X = np.sort(X).reshape(-1,1)\n",
    "\n",
    "print(X)\n",
    "\n",
    "w_groundtruth = 10\n",
    "Y_groundtruth = w_groundtruth*X\n",
    "\n",
    "epsilon_noise = np.random.normal(loc=0, scale=20.0, size=(num_datapoints,1))\n",
    "\n",
    "Y = Y_groundtruth + epsilon_noise "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa6166-8649-439d-85b6-3dfccc0ae334",
   "metadata": {},
   "source": [
    "A typical objective function is given by the **mean squared error** (MSE) loss\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Loss}(w) = \\frac{1}{2N} \\lVert \\epsilon \\rVert_2^2 = \\frac{1}{2N} \\lVert Y - Xw \\rVert_2^2 = \\frac{1}{2N} \\sum_{n=1}^{N} \\big( y_n -  x_n \\cdot w \\big)^2.\n",
    "\\end{equation}\n",
    "\n",
    "Under certain conditions, the solution to this equation will provide the **best linear (unbiased) estimator** for $w$! We identify the **minimum** through\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla  \\text{Loss}(w) = X^\\top Y - X^\\top X w \\equiv 0,\n",
    "\\end{equation}\n",
    "\n",
    "which provides the **ordinary least squares** (OLS) solution\n",
    "\n",
    "\\begin{equation}\n",
    "    w_\\text{OLS} = \\left( X^\\top X \\right)^{-1}X^\\top Y.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5c9bc-f14b-490e-a1ab-ea637fb6009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "XT = np.transpose(X)\n",
    "XTX_inverse = np.linalg.inv(XT @ X)\n",
    "\n",
    "w_OLS = XTX_inverse @ XT @ Y\n",
    "\n",
    "print(f\"The solution is given by w_OLS = {w_OLS.squeeze()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98059a17-f7e6-483d-808d-84b9fd7f121d",
   "metadata": {},
   "source": [
    "#### Note\n",
    "* Operator ```@``` provides the matrix multiplication of two numpy array (similar to ```np.matmul```)\n",
    "* Function ```numpy.array.squeeze()``` removes all dimensions of size ```1``` from a NumPy array, i.e.\n",
    "```python\n",
    "In: print(w_OLS.shape) \n",
    "Out: (1, 1)\n",
    "\n",
    "In: print(w_OLS)\n",
    "Out: [[10.04673211]]\n",
    "\n",
    "In: print(w_OLS.squeeze())\n",
    "Out: 10.046732109803889\n",
    "```\n",
    "\n",
    "Let's provide some predictions $Y_\\text{OLS}$ based on this solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96dfdba-38cd-4022-899c-e409d3adf4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_OLS = w_OLS * X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e3d86e-95b7-47ce-90bf-d9cab351821e",
   "metadata": {},
   "source": [
    "With this, we can calculate the **coefficient of determination** $R^2$ ([Wikipedia](https://en.wikipedia.org/wiki/Coefficient_of_determination)) through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43c5a0-ae97-410d-a58c-b929969047fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsquared = 1 - np.sum((Y - Y_OLS)**2) / np.sum((Y - np.mean(Y))**2)\n",
    "print(Rsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0446ee-0178-434e-b59e-b0d72cb42ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y, color='red', label='Observations (with noise)')\n",
    "plt.plot(X, Y_groundtruth, color='red', linestyle='dashed', label=f'Groundtruth')\n",
    "plt.plot(X, Y_OLS, color='blue', label=f'Fit with $R^2={Rsquared:.2f}$')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7297f46e-eb58-47bc-b1ff-335b03559e62",
   "metadata": {},
   "source": [
    "What happens if you have more \"flexible models\" at your disposal? In the following, we try to fit our observed \n",
    "responses $Y$ with a polynomial model of the form \n",
    "\n",
    "\\begin{align}\n",
    "    Y &= X_\\text{poly}w + \\epsilon \\\\ \n",
    "    &= x \\cdot w_1 + x^2 \\cdot w_2 + x^3 \\cdot w_3 + ... + x^7 \\cdot w_7 + x^8 \\cdot w_8 + \\epsilon\n",
    "\\end{align}\n",
    "\n",
    "and create a hundred test datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd02ac6-3528-480e-bdc2-455ee0a254b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly = np.hstack((X, X ** 2, X ** 3, X ** 4, X ** 5, X ** 6, X ** 7, X ** 8))\n",
    "print(X_poly.shape)\n",
    "\n",
    "X_test = np.linspace(10,50,100).reshape(-1, 1)\n",
    "X_test = np.hstack((X_test, X_test ** 2, X_test ** 3, X_test ** 4, X_test ** 5, X_test ** 6, X_test ** 7, X_test ** 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7aacf3-465f-42e7-b122-e49758a0caf4",
   "metadata": {},
   "source": [
    "Let's see the OLS solution for that case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f26363-330c-4643-9039-7afa875fd4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "XT = np.transpose(X_poly)\n",
    "XTX_inverse = np.linalg.inv(XT @ X_poly)\n",
    "\n",
    "w_OLS = XTX_inverse @ XT @ Y\n",
    "\n",
    "print(f\"Solution w_OLS = {w_OLS.squeeze()}\")\n",
    "\n",
    "Y_OLS = X_poly @ w_OLS\n",
    "Y_OLS_test = X_test @ w_OLS\n",
    "\n",
    "Rsquared = 1 - np.sum((Y - Y_OLS)**2) / np.sum((Y - np.mean(Y))**2)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "axs[0].scatter(X, Y, marker='o', color='red', label='Observations (with noise)')\n",
    "axs[0].plot(X, Y_groundtruth, color='red', linestyle='dashed', label=f'Groundtruth')\n",
    "axs[0].scatter(X, Y_OLS, color='blue', marker='X', label=f'Train; fit with $R^2={Rsquared:.2f}$')\n",
    "\n",
    "axs[1].scatter(X, Y, marker='o', color='red', label='Observations (with noise)')\n",
    "axs[1].plot(X, Y_groundtruth, color='red', linestyle='dashed', label=f'Groundtruth')\n",
    "axs[1].plot(X_test[:,0], Y_OLS_test, color='blue', label=f'Test')\n",
    "\n",
    "axs[0].set_xlabel('X')\n",
    "axs[0].set_ylabel('Y')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_xlabel('X')\n",
    "axs[1].set_ylabel('Y')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e8708-75b8-42a0-8d9d-f36a70b11a72",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "Ridge regression adds an additional assumption on (the distribution of) weights $w$. We rewrite the loss as\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Loss} (w) = \\frac{1}{2N} \\lVert Y - Xw \\rVert_2^2 \\color{red}{+ \\frac{1}{2} \\lambda' \\lVert w \\rVert^2_2} = \\frac{1}{2N} \\sum_{n=1}^{N} \\left( y_n - x_n \\cdot w \\right)^2 \\color{red}{+ \\frac{1}{2} \\lambda' \\sum_{i=1}^{d_{x}} w_i^2}\n",
    "\\end{equation}\n",
    "\n",
    "where the minimisation also takes into account the value (norm) of the weights $w_i$. Here, $d_x$ indicates the dimensions of our input $X$, e.g. $d_x=8$ in the case of the polynomial model above. The solution to this minimisation is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    w_\\text{ridge} = \\left(\\color{red}{\\lambda \\mathbb{1}_{d_x} +} X^\\top X \\right)^{-1} X^\\top Y.\n",
    "\\end{equation}\n",
    "\n",
    "Let's see what happens with this additional term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15a0ff3-bbc4-49bd-b98a-844f300d4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ridge = 100000\n",
    "\n",
    "XT = np.transpose(X_poly)\n",
    "XTX_ridge_inverse = np.linalg.inv(XT @ X_poly + lambda_ridge * np.identity(X_poly.shape[1]))\n",
    "\n",
    "w_ridge = XTX_ridge_inverse @ XT @ Y\n",
    "\n",
    "print(f\"Solutions: \\n w_OLS = \\t {w_OLS.squeeze()},\\n w_ridge = \\t {w_ridge.squeeze()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21dc28-0c47-4928-98a0-59efbbccd4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ridge = X_poly @ w_ridge \n",
    "Y_ridge_test = X_test @ w_ridge \n",
    "\n",
    "Rsquared_ridge = 1 - np.sum((Y - Y_ridge)**2) / np.sum((Y - np.mean(Y))**2)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "axs[0].scatter(X, Y, marker='o', color='red', label='Observations (with noise)')\n",
    "axs[0].plot(X, Y_groundtruth, color='red', linestyle='dashed', label='Groundtruth')\n",
    "axs[0].scatter(X, Y_OLS, color='blue', marker='X', label=f'Train; OLS with $R^2={Rsquared:.2f}$')\n",
    "axs[0].scatter(X, Y_ridge, color='green', marker='+', label=f'Train; ridge with $R^2={Rsquared_ridge:.2f}$')\n",
    "\n",
    "axs[1].scatter(X, Y, marker='o', color='red', label='Observations (with noise)')\n",
    "axs[1].plot(X, Y_groundtruth, color='red', linestyle='dashed', label='Groundtruth')\n",
    "axs[1].plot(X_test[:,0], Y_OLS_test, color='blue', label=f'Test; OLS')\n",
    "axs[1].plot(X_test[:,0], Y_ridge_test, color='green', label=f'Test; ridge')\n",
    "\n",
    "\n",
    "axs[0].set_xlabel('X')\n",
    "axs[0].set_ylabel('Y')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_xlabel('X')\n",
    "axs[1].set_ylabel('Y')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa92839-1fe0-481a-aa34-e1b365b3149d",
   "metadata": {},
   "source": [
    "#### Note\n",
    "that the additional term $\\lambda \\lVert w \\rVert^2_2 = \\lambda \\sum_{i=1}^{d_{x}=8} w_i^2$ \n",
    "in the loss function enables deviation from the observed noisy datapoints. That is models (i.e.\n",
    "weights $w_i$) which too closely *overfit* to the noisy observations are penalised. Parameter $\\lambda$\n",
    "serves as a **penalty factor** biasing towards *simpler* models which **reduce overfitting**.\n",
    "\n",
    "Generally, having terms in your loss function favouring simpler models is referred to as **regularisation** ([Wikipedia](https://en.wikipedia.org/wiki/Regularization_(mathematics))). Highly-flexible models (like the blue curve in the next figure) allow fitting observations (red dots) arbitrary well. Typically, we prefer to choose simpler models (like the green curve) which usually provide better predictions to new, unseen data (**generalisation**). \n",
    "\n",
    "<center><img src=\"images/Regularisation.png\" alt=\"Regularistation\" width=\"200\"/> <br> Image source: <a href=\"https://en.wikipedia.org/wiki/Regularization_(mathematics)#/media/File:Regularization.svg\">Wikipedia</a> <br> </center>\n",
    "\n",
    "***\n",
    "\n",
    "## Using scikit-learn for Regression\n",
    "\n",
    "You don't need to do all the work yourself! ```scikit-learn``` offers you the required functionality with the functions [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) and [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec7a8e-ac4e-4264-a5ed-e31f48fa3b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "linreg = LinearRegression(fit_intercept=False)\n",
    "linreg.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d7bb0a-d924-45e3-9918-ac9c299ab10e",
   "metadata": {},
   "source": [
    "#### Note \n",
    "that we used ```fit_intercept=False``` because we chose to neglect on off-set / intercept $b$ on the y-axis.\n",
    "That is, we consider $Y = Xw + b + \\epsilon$ with $b=0$, which simplifies writing the equations out.\n",
    "\n",
    "```linreg``` is the linear fit which provides you the same OLS solution as seen above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a740c-d0d0-4f11-aacb-b0fa2e9668a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w_OLS_sk = linreg.coef_\n",
    "Y_OLS_sk = linreg.predict(X)\n",
    "Rsquared_sk = linreg.score(X,Y)\n",
    "\n",
    "print(f\"w_OLS_sk = {w_OLS_sk.squeeze()}, Rsquared_sk = {Rsquared_sk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf62dadc-9ced-43d5-8d23-e5489ebc30e2",
   "metadata": {},
   "source": [
    "We can do the same for the ridge regression case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596589a-98e7-48fe-8de8-058f7d6d72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgereg = Ridge(fit_intercept=False)\n",
    "ridgereg.fit(X,Y)\n",
    "\n",
    "w_ridge_sk = ridgereg.coef_\n",
    "Y_ridge_sk = ridgereg.predict(X)\n",
    "Rsquared_ridge_sk = ridgereg.score(X,Y)\n",
    "\n",
    "print(f\"w_ridge_sk = {w_ridge_sk.squeeze()}, Rsquared_ridge_sk = {Rsquared_sk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa199b08-a918-4793-a960-94f7e474f735",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Logistic Regression for Classification\n",
    "\n",
    "Next, we consider binary classification where our data falls into one of two classes which we will denote\n",
    "with class labels $Y=0$ (class 1) and $Y=1$ (class 2). Let's assume that observations of \n",
    "class 2 occur with an unkown probability of $p_2$. As we only have to classes,\n",
    "the probablity for class 1 is $p_1 = 1 - p_2$. A natural choice in such a setting is the **Bernoulli distribution** \n",
    "([Wikipedia](https://en.wikipedia.org/wiki/Bernoulli_distribution)) with probability (mass) function\n",
    "\n",
    "\\begin{equation}\n",
    "    p(Y) = p_2^Y p_1^{1-Y} = p_2^Y (1-p_2)^{1-Y}.\n",
    "\\end{equation}\n",
    "\n",
    "The idea of logistic regression is to approximate the ratio of the two class probabilites with an exponential \n",
    "function of the form\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{p_2}{p_1} = \\exp (x_1 \\cdot w_1 + x_2 \\cdot w_1\\cdot w_2 + ... + x_{d_x} \\cdot w_{d_x}) = \\exp (Xw),\n",
    "\\end{equation}\n",
    "\n",
    "so again a linear model! Note that we can write this as\n",
    "\n",
    "\\begin{equation}\n",
    "     \\exp (Xw) = \\frac{p_2}{1-p_2} \\Rightarrow  p_2 = \\frac{1}{1+\\exp (-Xw)}.\n",
    "\\end{equation}\n",
    "\n",
    "This is the **logistic function**. In order to identify the weights $w$, i.e. a matching model, we use the **logistic loss** (also known as **negative log-likelihood** or **cross entropy loss**, depending on the context)\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Loss}(w) = - Y \\log p_2 - (1-Y) \\log (1-p_2) = - \\sum_{n=1}^{N} y_n \\log \\frac{1}{1+\\exp (-x_n \\cdot w)} + (1-y_n) \\log \\Big(1-\\frac{1}{1+\\exp (-x_n \\cdot w)}\\Big)\n",
    "\\end{equation}\n",
    "\n",
    "for $N$ datapoints. \n",
    "\n",
    "Let's create a simple binary classification problem with $50$ datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef4715-2680-45dd-a252-a43cf3f40eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "num_datapoints = 50\n",
    "\n",
    "X, Y = make_classification(n_samples=num_datapoints, n_features=1, n_informative=1, n_redundant=0, \n",
    "                           n_classes=2, n_clusters_per_class=1, weights=[0.3, 0.7],\n",
    "                           random_state=0)\n",
    "\n",
    "plt.scatter(X[Y == 0], Y[Y == 0], color='blue', marker='+', label='Class 1 encoded by Y=0')\n",
    "plt.scatter(X[Y == 1], Y[Y == 1], color='green', marker='x', label='Class 2 encoded by Y=1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2d413-6544-44ed-944a-1b1405bd0abb",
   "metadata": {},
   "source": [
    "Here we directly use ```scikit-learn``` function [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09963ea-c2c6-47cf-9bde-63c6e01ad441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd6ec6a-664b-424d-9e1b-83224409ecf3",
   "metadata": {},
   "source": [
    "Using the method ```predict_proba``` allows predicting the probability \n",
    "$p(Y=1)$ for class 2 of our logistic regression model ```logreg```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5f98a-aa3b-4f77-b19a-cdf0ccef6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_logreg = np.linspace(-2, 2, 1000).reshape(-1, 1)\n",
    "Y_logreg = logreg.predict_proba(X_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a7d298-70b6-45f8-b47c-54fc3945e06a",
   "metadata": {},
   "source": [
    "Let's plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29977c4c-dd39-4dcc-b789-ec064c2dc400",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[Y == 0], Y[Y == 0], color='blue', marker='+', label='Class 1 encoded by Y=0')\n",
    "plt.scatter(X[Y == 1], Y[Y == 1], color='green', marker='x', label='Class 2 encoded by Y=1')\n",
    "plt.plot(X_logreg[:, 0], Y_logreg[:, 1], color='red', label='Logistic Regression')\n",
    "plt.axhline(0.5, color='black', linestyle='--', label='Threshold')\n",
    "\n",
    "plt.xlabel('Predictor X')\n",
    "plt.ylabel(r'Class porbability $P(Y=1)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f85f20b-ef76-4f19-ba63-fdf3c468daa2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Decision Trees and Random Forests\n",
    "\n",
    "An more classical approach to classification problems are decision trees. \n",
    "In the next example, we will try to classify premium red wine from their \n",
    "measured propertes / features which serve as the predictors / covariates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749d377-f80b-40c6-8c58-6b0f49c24968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wine_data = pd.read_csv('data/winequality-red.csv', sep=';')\n",
    "\n",
    "quality_threshold = 6\n",
    "# quality_threshold = 5\n",
    "\n",
    "wine_data['premium'] = wine_data['quality'] > quality_threshold\n",
    "\n",
    "wine_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d4a5d-cf60-4d46-9d31-ebb27ff84c68",
   "metadata": {},
   "source": [
    "We focus on the quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf1e177-a0a0-462f-8ea5-62856d7bb87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_counts = wine_data['quality'].value_counts()\n",
    "\n",
    "if quality_threshold == 5:\n",
    "    colours = ['red','blue','blue','red','blue','red']\n",
    "elif quality_threshold == 6:\n",
    "    colours = ['red','red','blue','red','blue','red']\n",
    "\n",
    "plt.bar(quality_counts.index, quality_counts, color=colours)\n",
    "plt.xlabel('Quality assessment')\n",
    "plt.ylabel('Amount of different wines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bac517-2cfb-43b7-8042-2d5848f27768",
   "metadata": {},
   "source": [
    "Perpare the features and target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b632b-ba99-4933-8d49-e6997524b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = wine_data['premium'].astype(int)\n",
    "wine_features = wine_data.drop(['quality','premium'], axis=1)\n",
    "\n",
    "print(\"Shape of wine_features:\\t{}\\nShape of target:\\t{}\\n\"\n",
    "      .format(wine_features.shape, target.shape)\n",
    "     )\n",
    "\n",
    "target.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b80800-e177-4e20-b6eb-d948c72b8a5a",
   "metadata": {},
   "source": [
    "#### The __Goal__\n",
    "is to learn a classifier which predicts whether a wine is a premium /\n",
    "non-premium wine on the basis of the measured wine features.\n",
    "\n",
    "Select (randomly) a set on which the classifier is \n",
    "calibrated / trained on and a test set on which the performance\n",
    "is assessed. We consider a test set size of 30% of the original data\n",
    "set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab24dd-3f27-44df-8e71-32e2df155080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feat_train, feat_test, target_train, target_test = train_test_split(\n",
    "    wine_features, target, test_size = 0.3, random_state=123)\n",
    "\n",
    "print(\"After splitting into train and test sets:\\n\\n\"\n",
    "      f\"Shape of feat_train:\\t{feat_train.shape}\\nShape of target_train:\\t{target_train.shape}\\n\"\n",
    "      f\"Shape of feat_test:\\t{feat_test.shape}\\nShape of target_test:\\t{target_test.shape}\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4501a504-2fc5-400c-93e9-dc1ef24abf16",
   "metadata": {},
   "source": [
    "We have a first look on a decision tree and just consider the features \n",
    "providing the ```alcohol``` and ```sulphates``` content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0403fc8-6585-4af1-8ef1-a6ed8b4dd6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "\n",
    "feat_train_subset = feat_train[['alcohol','sulphates']]\n",
    "\n",
    "wine_tree = DecisionTreeClassifier(max_depth=2)\n",
    "wine_tree.fit(feat_train_subset, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe262a32-72dd-45df-85cc-7e11305bc256",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "try:\n",
    "    from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "    display = DecisionBoundaryDisplay.from_estimator(\n",
    "        wine_tree,\n",
    "        feat_train_subset,\n",
    "        cmap='RdYlBu',\n",
    "        response_method=\"predict\",\n",
    "        ax=axs[0],\n",
    "        xlabel='alcohol',\n",
    "        ylabel='sulphates',\n",
    "    )\n",
    "except:\n",
    "    print(\"Sorry, the library for plotting the decision \" \n",
    "          \"boundary is currently missing\")\n",
    "\n",
    "plot_tree(wine_tree, max_depth=2, \n",
    "          feature_names=['alcohol','sulphates'], \n",
    "          class_names=['non-premium','premium'],\n",
    "          ax=axs[1], fontsize=7, filled = True,\n",
    "         )\n",
    "\n",
    "\n",
    "axs[0].scatter(feat_train['alcohol'], feat_train['sulphates'],\n",
    "               c=target_train, cmap='RdYlBu', edgecolor='black'\n",
    "              )\n",
    "\n",
    "plt.tight_layout()                        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc58a9b-8cd9-4b70-99ca-b3737350421d",
   "metadata": {},
   "source": [
    "#### Note \n",
    "that decision trees partition the input space into different regions with \n",
    "**decision boundaries** separating different classes, like premium wines \n",
    "(blue data / region) and non-premium wines (red data / region)!\n",
    "\n",
    "Now let's consider the full dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5fa222-e23c-495d-aa8d-425250006e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_height = 8\n",
    "\n",
    "wine_tree = DecisionTreeClassifier(max_depth=tree_height)\n",
    "wine_tree.fit(feat_train, target_train)\n",
    "\n",
    "correct_pred = wine_tree.predict(feat_test) == target_test\n",
    "\n",
    "correct = correct_pred.value_counts()\n",
    "\n",
    "accuracy = (correct[True] / (correct[True] + correct[False]))*100\n",
    "print(f\"The random forest identified premium / non-premium wines with {accuracy}% accuracy!\")\n",
    "\n",
    "plot_tree(wine_tree, \n",
    "          feature_names=wine_features.columns, \n",
    "          class_names=['non-premium','premium'],\n",
    "          filled = True,\n",
    "         )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77143e-28b7-4064-b056-87844844dd96",
   "metadata": {},
   "source": [
    "#### **Random Forests**\n",
    "are now very powerful machine learning methods which build on decision trees. \n",
    "Random Forests construct a multitude of decision trees like the one above. \n",
    "The individual decision trees provide a classification result and by majority \n",
    "voting the final classification is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0c395-54e3-45b3-8863-8848ef5addf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "num_trees = 100\n",
    "tree_height = 8\n",
    "\n",
    "wine_forest = RandomForestClassifier(n_estimators=num_trees, \n",
    "                                       max_depth=tree_height, \n",
    "                                       random_state=123)\n",
    "wine_forest.fit(feat_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8af5325-84e0-4054-bae7-bbc141fab016",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = wine_forest.predict(feat_test) == target_test\n",
    "\n",
    "correct = correct_pred.value_counts()\n",
    "\n",
    "accuracy = (correct[True] / (correct[True] + correct[False]))*100\n",
    "print(f\"The random forest identified premium / non-premium wines with {accuracy}% accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c262762-cb4c-48cc-a840-2317c7fd633e",
   "metadata": {},
   "source": [
    "#### That's a great accuracy! But\n",
    "let's check what is actually predicted wrongly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b37a4-7df0-4c4b-855f-11ad4e981581",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_incorrect_pred = target_test[correct_pred == False].value_counts() / target_test.value_counts()\n",
    "\n",
    "print(f\"Incorrect predictions by premium quality:\\n{rel_incorrect_pred}\")\n",
    "print(f\"\\nRatio of premium / non-premium wines in test set:\\n{target_test.value_counts(normalize=True)}\")      \n",
    "print(f\"\\nRatio of premium / non-premium wines in train set:\\n{target_train.value_counts(normalize=True)}\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc57bc88-6865-4a6e-959a-6c4ee79d99eb",
   "metadata": {},
   "source": [
    "You can still plot one of the trees!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce4983-8607-4a81-9327-9f827321d1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_tree = wine_forest.estimators_[5]\n",
    "\n",
    "plot_tree(single_tree, \n",
    "          feature_names=wine_features.columns, \n",
    "          class_names=['non-premium','premium'],\n",
    "          filled = True,\n",
    "         )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c1f6f7-a9b8-49f7-b78c-afd96c9609e4",
   "metadata": {},
   "source": [
    "However, this is less insightful, as the decision is made by vote of many (different) trees!\n",
    "Still, we can study the **feature importance score**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d368cb6-fa29-4ab2-b56a-022cbef9bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = wine_forest.feature_importances_\n",
    "feature_names = list(wine_features.columns)\n",
    "\n",
    "important_features = pd.Series(feature_scores, index=feature_names).sort_values()\n",
    "\n",
    "plt.barh(important_features.index, important_features, color='Orange')\n",
    "plt.xlabel('Feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64fc421-3bad-45d3-b4c9-609dcdc99a5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "## Exercise Section\n",
    "\n",
    "(1.) In this exercise, we train a ```Ridge``` regressor for predicting the ```quality``` values on the test set ```feat_test```. \n",
    "First, load the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a7868-b8ee-42ce-82ea-b5e117f927ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine_data = pd.read_csv('data/winequality-red.csv', sep=';')\n",
    "\n",
    "ex_target = wine_data['quality']\n",
    "ex_features = wine_data.drop(['quality'], axis=1)\n",
    "\n",
    "feat_train, feat_test, target_train, target_test = train_test_split(\n",
    "    ex_features, ex_target, test_size = 0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98c1f0-7c8d-4487-afc8-8d39be2e0ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feat_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf096b9-8a19-40fe-b698-65452ce3aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0cd99-4058-41e8-b79a-06752e1f0358",
   "metadata": {},
   "source": [
    "Put your result in the next cell and use ```ex_pred_ridge``` for the predicted quality values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed4296e-a8cf-4998-8b5f-173bbf775e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Fill in\n",
    "\n",
    "ex_pred_ridge = # Fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92633f48-dba6-4b67-82d6-50239be0d28d",
   "metadata": {},
   "source": [
    "Execute the next cell to save the results in a summary data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7cf3ee-d214-449e-bff5-b530affe9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test = target_test.to_frame()\n",
    "target_test['Ridge_predicted_quality'] = np.around(ex_pred_ridge, decimals=2)\n",
    "target_test['Ridge_absolut_deviation'] = abs(target_test['quality'] - target_test['Ridge_predicted_quality'])\n",
    "target_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638620c7-c7cb-4a43-9c52-22104c01cfce",
   "metadata": {},
   "source": [
    "(2.) You can create a Random Forest not only for classification, but also regression. Make use of the ```scikit-learn``` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e102138-2d17-48e5-9459-528f565ae1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf4697-2ae8-4455-b901-9eb87acf678f",
   "metadata": {},
   "source": [
    "to make predictions on the wine quality based on the other features / predictors. Load the previous cell\n",
    "and train a the ```RandomForestRegressor``` for predicting the quality values on the test set ```feat_test```. Put your result in the next cell and use ```ex_pred_RF``` for the predicted quality values.\n",
    "\n",
    "Hint: You might want to revisit the steps for RF classification we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d880e0-62df-45d8-9e12-a0850d474944",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees = 100\n",
    "tree_height = 8\n",
    "\n",
    "# Fill in\n",
    "\n",
    "ex_pred_RF = # Fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dc2fec-ee1a-4b0f-a24a-5ae1c2ec1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test['RF_predicted_quality'] = np.around(ex_pred_RF, decimals=2)\n",
    "target_test['RF_absolut_deviation'] = abs(target_test['quality'] - target_test['RF_predicted_quality'])\n",
    "target_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1769f2-4942-4c75-94ac-01c90a83f4c5",
   "metadata": {},
   "source": [
    "Finally, compare how the Random Forest and ridge regressor performed in comparison.\n",
    "For this, just execute the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099af4d-4ef6-46bb-a147-fbbf6b239dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_pred_MSE = (target_test['RF_absolut_deviation']**2).mean()\n",
    "Ridge_pred_MSE = (target_test['Ridge_absolut_deviation']**2).mean()\n",
    "\n",
    "print(f\"Mean squared error of RandomForestRegressor: {RF_pred_MSE:.2f}\")\n",
    "print(f\"Mean squared error of Ridge: {Ridge_pred_MSE:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
