{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6a8128-fd85-4b58-8c1d-b8c5456d64b5",
   "metadata": {},
   "source": [
    "# 7. Image Segmentation Example\n",
    "\n",
    "In this last notebook, we see a medical imaging example for which we can make use of neural networks. To this end, we consider the [LIDC-IDRI](https://pubmed.ncbi.nlm.nih.gov/21452728/) dataset of lung CT scans where anomalities, in particular lesions, were manually outlined by experts. The goal is to design a **neural network for image segmentation** \n",
    "which learns from previous outlines of lesions to predict anomalities in new lung CT scans.\n",
    "\n",
    "Keywords: ```Semantic segmentation```, ```U-Net```, ```keras.layers.SeparableConv2D```, ```keras.layers.BatchNormalization```, ```keras.layers.UpSampling2D```\n",
    "\n",
    "***\n",
    "\n",
    "## U-Net-based Semantic Segmentation\n",
    "\n",
    "**Semantic segmentation** describes the task of grouping pixels together which (semanticly) belong to one object. In that regrad, a particularly useful architecture for biomedical imaging tasks is the [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) illustrated in the following figure. It is a **fully-convolutional neural network** and can be used also with comparably small datasets.\n",
    "\n",
    "<br><center><img src=\"images/U-Net.png\" alt=\"U-Net\" width=\"500\"/></center></br>\n",
    "\n",
    "The architecture we use in this notebook is based on the U-Net. We adopt the *U-Net Xception-style model* proposed in a [Keras tutorial](https://keras.io/examples/vision/oxford_pets_image_segmentation/) on image segmentation.\n",
    "\n",
    "With the following cell, we load and plot the dataset. Note that we only use a small subset (119 images) of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15c4c0-96d9-46e8-adaf-3f0ec1b12835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images = np.load('data/lidc_images_119.npy')\n",
    "targets = np.load('data/lidc_labels_119.npy')\n",
    "\n",
    "fig, axs = plt.subplots(10,2, figsize=(4,20))\n",
    "\n",
    "for i in range(10):\n",
    "    axs[i,0].imshow(images[i])\n",
    "    axs[i,1].imshow(targets[i])\n",
    "    \n",
    "    axs[i,0].set_xticks([])\n",
    "    axs[i,0].set_yticks([])\n",
    "    axs[i,1].set_xticks([])\n",
    "    axs[i,1].set_yticks([])\n",
    "    \n",
    "axs[0,0].set_title('Image')\n",
    "axs[0,1].set_title('Groundtruth')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "train_images = images[:100][...,np.newaxis]\n",
    "train_targets = targets[:100][...,np.newaxis]\n",
    "\n",
    "test_images = images[100:][...,np.newaxis]\n",
    "test_targets = targets[100:][...,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c560d5-eb9c-45ca-8d67-43f12ce317e1",
   "metadata": {},
   "source": [
    "We use the following hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f4602-4645-459f-b340-a403c64d8392",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2c139-8376-4c23-b2e9-8065cd753b3a",
   "metadata": {},
   "source": [
    "and define the model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec15d54-5d3b-4883-be6c-1d2aa1a1b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "keras.utils.set_random_seed(123)\n",
    "\n",
    "def segmentation_model(img_size=(128, 128), num_classes=2):\n",
    "    inputs = keras.Input(shape=img_size + (1,))\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = keras.layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = keras.layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = keras.layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.Activation(\"relu\")(x)\n",
    "        x = keras.layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = keras.layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = keras.layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = keras.layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = keras.layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = keras.layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7124ae00-49ec-458b-9333-9d8ca67233d1",
   "metadata": {},
   "source": [
    "Now, we build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2047c-c691-415c-8114-f5cc1a4139a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = segmentation_model(img_size=(128, 128), num_classes=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b99db-d1a1-4205-a469-800115875891",
   "metadata": {},
   "source": [
    "With all of this in place, we can train our model. Note that\n",
    "**this can take a substantial time**, depending on your machine and\n",
    "the total amount of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6772660-09ce-4e65-b88c-064ae0085156",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "train_history = model.fit(x=train_images, y=train_targets, batch_size=batch_size, \n",
    "                          validation_data = (test_images, test_targets),\n",
    "                          epochs=num_epochs)\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(9,3))\n",
    "\n",
    "axs[0].plot(train_history.history['accuracy'], label='Training')\n",
    "axs[0].plot(train_history.history['val_accuracy'], label='Validation')\n",
    "axs[0].set_title('Accuracy')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(train_history.history['loss'])\n",
    "axs[1].plot(train_history.history['val_loss'])\n",
    "axs[1].set_title('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e52788-630b-4de8-9b30-ffacbf988331",
   "metadata": {},
   "source": [
    "Save the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76aa6b-224d-493b-b731-a783b6594a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('output/Segmentation_model.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea77ec2-7fc1-45b1-b415-b59012c9d205",
   "metadata": {},
   "source": [
    "We provide again a pretrained model (500 epochs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ca875-4f22-4a28-89a2-2d1eb5bdf967",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('output/Segmentation_model_pretrained.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a0ace8-c947-4b15-bfe1-58c0d7428664",
   "metadata": {},
   "source": [
    "Make model prediction for both the training and test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80157f10-eebc-4682-8090-830c910038c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction = model.predict(train_images)\n",
    "test_prediction = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd27d60-a9c4-4fee-9a26-34619d12fe36",
   "metadata": {},
   "source": [
    "First, we examine how well the model predicts the segmentation result on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf52a88-8253-4f7e-a54f-cce30e620a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(10,3, figsize=(6,20))\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    axs[i,0].imshow(train_images[i])\n",
    "    axs[i,1].imshow(train_targets[i])\n",
    "    axs[i,2].imshow(train_prediction[i,...,1])\n",
    "    \n",
    "    axs[i,0].set_xticks([])\n",
    "    axs[i,0].set_yticks([])\n",
    "    axs[i,1].set_xticks([])\n",
    "    axs[i,1].set_yticks([])\n",
    "    axs[i,2].set_xticks([])\n",
    "    axs[i,2].set_yticks([])\n",
    "    \n",
    "axs[0,0].set_title('Image')\n",
    "axs[0,1].set_title('Groundtruth')\n",
    "axs[0,2].set_title('Prediction')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0597e1f8-102e-4c28-b6e3-1765f6ffa4b9",
   "metadata": {},
   "source": [
    "Lastly, let's examine the predictions of segments on the test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a39d4-1879-46bd-84a1-8d1d8bbbe8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(10,3, figsize=(6,20))\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    axs[i,0].imshow(test_images[i])\n",
    "    axs[i,1].imshow(test_targets[i])\n",
    "    axs[i,2].imshow(test_prediction[i,...,1])\n",
    "    \n",
    "    axs[i,0].set_xticks([])\n",
    "    axs[i,0].set_yticks([])\n",
    "    axs[i,1].set_xticks([])\n",
    "    axs[i,1].set_yticks([])\n",
    "    axs[i,2].set_xticks([])\n",
    "    axs[i,2].set_yticks([])\n",
    "    \n",
    "axs[0,0].set_title('Image')\n",
    "axs[0,1].set_title('Groundtruth')\n",
    "axs[0,2].set_title('Prediction')\n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
