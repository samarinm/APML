{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233ab466-39a1-4979-add4-d34effd5ced4",
   "metadata": {},
   "source": [
    "# 5. Neural Networks\n",
    "\n",
    "In the previous notebook, we have seen that we can choose feature mappings $\\phi$ or kernels $\\kappa$\n",
    "to design powerful non-linear models. Informally, deep learning can be characterised as trying to\n",
    "learn the feature mapping $\\phi$ from data.\n",
    "\n",
    "In this notebook we will discuss and implement our first\n",
    "\n",
    "* feed-forward neural network / multi-layer perceptron and\n",
    "* convolutional neural network\n",
    "\n",
    "for the MNIST and wine quality classification task.\n",
    "\n",
    "Keywords: ```Gradient descent```, ```Cross-entropy loss```, ```ReLU```, ```One-hot encoding```, \n",
    "```Max pooling```, ```Convolution```, ```keras.layers```, ```model.compile```, ```model.fit```, ```model.evaluate```,  ```model.save```, ```model.summary```, ```models.load_model```, ```keras.optimizers.SGD```\n",
    "\n",
    "***\n",
    "\n",
    "## Neural Network Basics\n",
    "\n",
    "We start with the classical **Feed-Forward Neural Networks (FFNNs)** which are also referred to as multi-layer \n",
    "perceptrons (MLPs). FFNNs were originally proposed as a computational model \n",
    "simulating mechanisms of the human brain. The general idea is that inputs\n",
    "are processed in a sequential manner by **artificial neurons**. The common architecture consist of an\n",
    "input layer, one to several hidden layers, and an output layer as illustrated in the following \n",
    "figure. \n",
    "\n",
    "<center><img src=\"images/NN_idea.png\" alt=\"Neural Network Illustration\" width=\"450\"/></center>\n",
    "\n",
    "As before, we try to learn the function $f(X)$. The displayed three-layer FFNN **parameterises** this\n",
    "function in the following way\n",
    "\n",
    "\\begin{equation}\n",
    "    f(X,W) =  \\sigma \\big( \\sigma (X W_1 ) W_2 \\big) W_3 = \\phi (X,W_1,W_2) W_3, \n",
    "\\end{equation}\n",
    "\n",
    "where weights $W = (W_1, W_2, W_3)$ are the matrices which we multiply layer-wise to the output of the\n",
    "previous layer. The outputs are called **activations**. An important part of why neural networks \n",
    "are so powerful is the use of **non-linear activation functions**. The \n",
    "**rectified linear unit (ReLU)** $\\sigma (z) = \\max \\{0, z \\}$ is a particularly popular choice,\n",
    "e.g. $\\sigma (z=-2) = 0$ and $\\sigma (z=2) = 2$, with $z$ being the preactivation.\n",
    "\n",
    "<center><img src=\"images/ReLU.png\" alt=\"ReLU\" width=\"250\"/></center>\n",
    "\n",
    "We can make a connection between neural networks and what we have seen before. Let's consider\n",
    "a deeper $L$-layer FFNN. In the regression setting, our goal is again to find a model for\n",
    "\n",
    "\\begin{equation}\n",
    "    Y = f(X,W) + \\epsilon = \\phi(X,W_1, \\dots, W_{L-1} ) W_L + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "where this time $\\phi$ is learned! We calibrate our model to solve the task \n",
    "by using **(Stochastic) Gradient Descent** for updating the weights $W = (W_1,\\dots, W_L)$:\n",
    "\n",
    "\\begin{equation}\n",
    "    W^{(t+1)} = W^{(t)} - \\eta \\nabla \\text{Loss}(W^{(t)}).\n",
    "\\end{equation}\n",
    "\n",
    "As illustrated in the figure below, the idea is to start at some position of our loss\n",
    "landscape described by initially random weights $W^{(t=0)}$ drawn from a Gaussian distribution.\n",
    "With every update step, we approach a (local) minimum until the weights do not change any longer.\n",
    "Note that this requires an appropriate choice of the step size or **learning rate** $\\eta$ (which\n",
    "again is a hyperparameter like $\\lambda$ and $k$ which we've seen before).\n",
    "\n",
    "<center><img src=\"images/Gradient_Descent.png\" alt=\"Gradient Descent\" width=\"350\"/></center>\n",
    "\n",
    "As before, we can use the mean-squared error loss for regression tasks\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Loss}(W) = \\frac{1}{2N} \\lVert Y - f(X,W) \\rVert_2^2 = \\frac{1}{2N} \\sum_{n=1}^{N} \\big( y_n -  f(x_n,W) \\big)^2.\n",
    "\\end{equation}\n",
    "\n",
    "In the classification setting, we use the multi-class version of the logistic loss encountered\n",
    "earlier. In deep learning context, we usually call it the **cross-entropy loss**\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Loss}(W) = - \\sum_{n=1}^{N} \\sum_{c=1}^{C} y_{n,c} \\log p_{n,c} = - \\sum_{n=1}^{N} \\sum_{c=1}^{C} y_{n,c} \\log f_c (x_n,W),\n",
    "\\end{equation}\n",
    "\n",
    "where we consider $C$ different classes our $N$ datapoints belong to. The neural network provides \n",
    "in the output layer the probabilities for the different classes $p_{n,c}=f_c (x_n,W)$. For this, we use\n",
    "another kind of activation function knows as the **softmax activation function** which maps the activation\n",
    "to the values between 0 and 1. The activation $\\sigma(z)_i$\n",
    "at output neuron $i$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sigma(z)_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{C} \\exp (z_j)},\n",
    "\\end{equation}\n",
    "\n",
    "where $C$ is the number of neurons in the output layer, i.e. the number of classes, and $i,j\\in {1,\\dots,C}$.\n",
    "Typically, we change the class labels into a **one-hot encoding** which provides something like a class probability associated with the label. Suppose our datapoint $x_n$ shows the digit 9 and the corresponding\n",
    "target is $y_n=9$. In the one-hot encoding we replace $y_n$ with the vector\n",
    "\n",
    "\\begin{equation}\n",
    "    y_{n} = \\begin{pmatrix} y_{n,0} \\\\ y_{n,1} \\\\ \\vdots \\\\ y_{n,9} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "where we have 0s at every index but the class index, i.e. 9 in this case.\n",
    "\n",
    "Let's use all of this in building a FFNN for classifying MNIST digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526f956-c910-4fbf-8ada-7ce3ed5386fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "mnist_data = np.load('data/mnist_data_5k.npy', allow_pickle=True)\n",
    "mnist_targets = np.load('data/mnist_labels_5k.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b016a-00f4-4f22-9951-2c7444614f4f",
   "metadata": {},
   "source": [
    "In the following, we normalise the vector of pixel values to \n",
    "the $[0,1]$ interval. It is typically advisable to scale all your\n",
    "predictors in $X$ (i.e. the columns) such that values range between 0 and 1.\n",
    "Furthermore, we split the data into training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e563e268-b49c-4233-9504-1749faa62a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mnist_data_flat = mnist_data.reshape(-1, 784, 1) / 255\n",
    "\n",
    "num_train = 3000\n",
    "num_val = 1000\n",
    "num_test = 1000\n",
    "\n",
    "x_train = mnist_data_flat[:num_train]\n",
    "y_train = mnist_targets[:num_train]\n",
    "\n",
    "x_val = mnist_data_flat[num_train:num_train+num_val]\n",
    "y_val = mnist_targets[num_train:num_train+num_val]\n",
    "\n",
    "x_test = mnist_data_flat[-num_test:]\n",
    "y_test = mnist_targets[-num_test:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63abf8d-7fd2-4930-a39a-349e2531bf83",
   "metadata": {},
   "source": [
    "Now, we define a three-layer FFNN with the following layers sizes:\n",
    "\n",
    "* 784 neurons in the input layer\n",
    "* 1024 (hidden) neurons in the first hidden layer with ReLU activation\n",
    "* 512 (hidden) neurons in the second hidden layer with ReLU activation\n",
    "* 10 output neurons for our classes in the output layer with a softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476fedc-2133-4b68-9441-257757c0ed2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ffnn = keras.Sequential([\n",
    "    keras.layers.Dense(1024, activation='relu', input_shape=(784,)),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "    ], name='FFNN_example')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8801e2-9d2e-46ab-8ba7-1a01dbd3ef20",
   "metadata": {},
   "source": [
    "Next, we specify hyperparameters. As outlined before, the **learning rate** $\\eta$\n",
    "is an important hyperparameter. In addition, we typically cannot (but also do not want to) \n",
    "use all datapoints for training at the same time. We choose small batches of the dataset\n",
    "and thus need to specify the **batch size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e8e8f-2153-48a6-ad5f-ee341134377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "\n",
    "print(f\"Shape of training set: {x_train.shape}\")\n",
    "print(f\"Number of batches: {x_train.shape[0] / batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c0f24-3fd8-4e34-b20d-ad38d2e93d24",
   "metadata": {},
   "source": [
    "A training **epoch** is concluded after we have seen the total number of batches.\n",
    "We set multiple epochs to iteratively refine the neural network weights, i.e. training\n",
    "samples are seen several times during training.\n",
    "\n",
    "Now, let's define our optimisation method where we use [Stochastic Gradient Descent](https://keras.io/api/optimizers/sgd/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14787b6-7bad-49d3-9a8f-03ec7c1ed8f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(123)\n",
    "\n",
    "ffnn.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418b67da-e19d-47c0-986c-d9c28c4262d9",
   "metadata": {},
   "source": [
    "We use as the loss the ```sparse_categorical_crossentropy``` where\n",
    "\n",
    "* *categorical* means that the neural network outputs are expected to be softmax values between 0 and 1\n",
    "* *sparse* indicates that we have did not one-hot encode our y-targets; the loss will take care of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83222c-8fe5-4277-9abc-5b2ca569ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbc3dcf-aecf-47f3-847d-abe9c84a0754",
   "metadata": {},
   "source": [
    "We can now use the methods [fit](https://keras.io/api/models/model_training_apis/#fit-method) and [evaluate](https://keras.io/api/models/model_training_apis/#evaluate-method) to train our FFNN\n",
    "and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4892a-cac1-4f50-bfe6-1618d18de32d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ffnn_history = ffnn.fit(x=x_train, y=y_train, batch_size=batch_size, \n",
    "                        validation_data=(x_val, y_val), epochs=num_epochs)\n",
    "\n",
    "test_loss, test_acc = ffnn.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'\\nTest accuracy: {test_acc:.4f}')\n",
    "\n",
    "ffnn.save('output/MNIST_FFNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19deeebc-3ecc-489b-9ff0-582367904229",
   "metadata": {},
   "source": [
    "We can summarise the network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586affc-cdec-406f-9ffe-e5bbbc0ffa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1688aa7f-91f9-4ac4-9e0e-ad40b8c00d8f",
   "metadata": {},
   "source": [
    "Let's see how the accuracy and loss developped over training making\n",
    "use of the ```ffnn_history``` object we assigned after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c536f-4532-4832-b9fe-5452943d70d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(9,3))\n",
    "\n",
    "axs[0].plot(ffnn_history.history['accuracy'], label='Training')\n",
    "axs[0].plot(ffnn_history.history['val_accuracy'], label='Validation')\n",
    "axs[0].set_title('Accuracy')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(ffnn_history.history['loss'])\n",
    "axs[1].plot(ffnn_history.history['val_loss'])\n",
    "axs[1].set_title('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a628169-34d0-4663-bf30-8b7defb34b8e",
   "metadata": {},
   "source": [
    "We use the [predict](https://keras.io/api/models/model_training_apis/#predict-method)\n",
    "method to provide predictions to new data. You can use the model you just trained or you\n",
    "can load a previously **pretrained** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f960da9-bb29-402d-9eb3-6b9170cd9921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Our pretrained model (100 epochs):\n",
    "# ffnn = keras.models.load_model('output/MNIST_FFNN_pretrained.h5')\n",
    "\n",
    "# ... or load the model you trained yourself:\n",
    "ffnn = keras.models.load_model('output/MNIST_FFNN.h5')\n",
    "\n",
    "test_loss, test_acc = ffnn.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test accuracy: {test_acc:.4f}\\n')\n",
    "\n",
    "num_examples = 10\n",
    "\n",
    "predictions_1hot = ffnn.predict(x_test, verbose=0)\n",
    "\n",
    "nicer_predictions_1hot_0 = list(map(np.format_float_positional, predictions_1hot[0], [5]*10))\n",
    "\n",
    "print(f\"Shape of the predictions: {predictions_1hot.shape}\\n\")\n",
    "print(f\"First prediction: {nicer_predictions_1hot_0}\\n\")\n",
    "\n",
    "predictions = np.argmax(predictions_1hot, axis=1)\n",
    "\n",
    "print(f\"First class prediction: {predictions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904c2d97-c0d1-4bcc-aa05-b7fae3679410",
   "metadata": {},
   "source": [
    "Note that the neural network outputs vectors with class probabilities.\n",
    "We choose the class with the highest probability.\n",
    "\n",
    "Let's plot some of the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe02c5-1114-470f-b942-4471883ba5f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,num_examples, figsize=(num_examples,5))\n",
    "\n",
    "start_index = 900\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(x_test[start_index+i].reshape(28,28))    \n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f\"Target: {y_test[start_index+i]}\\nPred: {predictions[start_index+i]}\", \n",
    "                 fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2495b54f-88c6-4b5a-be24-5746f01ab8aa",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "An important class of neural network architectures are **convolutional neural networks (CNNs)**. CNNs are\n",
    "inspired by biological models for the primary visual cortex and their resemblance to the mathemtical convolution \n",
    "operation. CNNs can be viewed as a particular kind of FFNNs where certain connections are missing and some\n",
    "neurons share the same weights. This gives rise to architectures as illustrated in the following figure.\n",
    "\n",
    "<center><img src=\"images/CNN_example.png\" alt=\"Convolutional Neural Network (LeNet-5)\" width=\"650\"/></center>\n",
    "\n",
    "CNNs are particularly suited to work with images and generally data **where some kind of local\n",
    "relationship / correlation** are present, e.g. adjacent pixels in an image or adjacent time steps in time series data. Two key components of CNNs are pooling and convolutional layers illustrated in the \n",
    "following.\n",
    "\n",
    "<center><img src=\"images/Max-pool_Conv.png\" alt=\"Max Pooling and Convolution Operator\" width=\"600\"/></center>\n",
    "\n",
    "**Convolutional layers** use a **convolutional filter** (sometimes called *kernel*, too) which processes the input images with respect to particular features (like edges) producing **feature maps**. Sequentially applying convolutional layers allow for **identifying increasingly complex features**. A typical size of a convolutional filter is $3 \\times 3$ which can be thought of as a matrix of weights (in red above) which are adjusted during training to identify features in a data-driven fashion.\n",
    "\n",
    "**Pooling layers** reduce the resolution of feature maps and don't have any associated weights. In particular, \n",
    "max pooling layers of size $2 \\times 2$ are used to identify the main activation in adjecent pixels.\n",
    "This allows objects to be in different positions in the image but still be recognised as the same object.\n",
    "\n",
    "Let's implement such a CNN and reshape the image vectors to actual images again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e528c-9930-460e-ad56-bd6e77efd84d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mnist_data_image = mnist_data.reshape(-1, 28, 28, 1) / 255\n",
    "\n",
    "x_train = mnist_data_image[:num_train]\n",
    "x_val = mnist_data_image[num_train:num_train+num_val]\n",
    "x_test = mnist_data_image[-num_test:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deabf9f9-06fb-474c-90ea-8bc11cee305b",
   "metadata": {},
   "source": [
    "We define a CNN with the following layers and train as before:\n",
    "\n",
    "* 28 $ \\times $ 28 $ \\times $ 1 neurons in the input layer for the greyscale image\n",
    "* 6 conv. filters of size $3 \\times 3$ in the first hidden layer with ReLU activation followed by a $2 \\times 2$ max pooling\n",
    "* 16 conv. filters of size $3 \\times 3$ in the first hidden layer with ReLU activation followed by a $2 \\times 2$ max pooling\n",
    "* flatten these feature maps in a single vector \n",
    "* 1024 (hidden) neurons in the second hidden layer with ReLU activation\n",
    "* 512 (hidden) neurons in the second hidden layer with ReLU activation\n",
    "* 10 output neurons for our classes in the output layer with a softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9151a-1adf-4b2f-975a-d0fec9fbe99e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(123)\n",
    "\n",
    "cnn = keras.Sequential([\n",
    "    keras.layers.Conv2D(6, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(16, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1024, activation='relu'),\n",
    "    keras.layers.Dense(512, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "    ], name='CNN_example') \n",
    "\n",
    "cnn.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "cnn_history = cnn.fit(x=x_train, y=y_train, batch_size=batch_size, \n",
    "                      validation_data=(x_val, y_val), epochs=num_epochs)\n",
    "\n",
    "test_loss, test_acc = cnn.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'\\nTest accuracy: {test_acc:.4f}\\n')\n",
    "\n",
    "cnn.save('output/MNIST_CNN.h5')\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6fbe8c-e6a7-4611-9d35-89d0fb6f0153",
   "metadata": {},
   "source": [
    "Let's study the performance of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06304208-66af-4f3c-a2fd-71d8bf98fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(9,3))\n",
    "\n",
    "axs[0].plot(cnn_history.history['accuracy'], label='Training')\n",
    "axs[0].plot(cnn_history.history['val_accuracy'], label='Validation')\n",
    "axs[0].set_title('Accuracy')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(cnn_history.history['loss'])\n",
    "axs[1].plot(cnn_history.history['val_loss'])\n",
    "axs[1].set_title('Loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Our pretrained model (100 epochs):\n",
    "# cnn = keras.models.load_model('output/MNIST_CNN_pretrained.h5')\n",
    "\n",
    "# ... or load your model:\n",
    "cnn = keras.models.load_model('output/MNIST_CNN.h5')\n",
    "\n",
    "test_loss, test_acc = cnn.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test accuracy: {test_acc:.4f}\\n')\n",
    "\n",
    "num_examples = 10\n",
    "\n",
    "predictions_1hot = cnn.predict(x_test, verbose=0)\n",
    "predictions = np.argmax(predictions_1hot, axis=1)\n",
    "\n",
    "fig, axs = plt.subplots(1, num_examples, figsize=(num_examples,5))\n",
    "\n",
    "start_index = 900\n",
    "\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    ax.imshow(x_test[start_index+i])    \n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f\"Target: {y_test[start_index+i]}\\nPred: {predictions[start_index+i]}\", \n",
    "                 fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f850c-bbe0-4a88-b602-6219b5b921b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "## Exercise Section\n",
    "\n",
    "For a final time, we consider the red wine dataset with our new regression method.\n",
    "As before, we want to predict the ```quality``` values on the test set ```feat_test``` \n",
    "and compare it to the previous results:\n",
    "\n",
    "* ridge regression MSE 0.42 \n",
    "* Random Forest regression MSE 0.34\n",
    "* kernel ridge regression MSE 0.38 \n",
    "* Gaussian Process regression MSE 0.38\n",
    "\n",
    "Prepare the data by load the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd08845-41fc-4f23-9211-e70a152a28b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "np.random.seed(123)\n",
    "keras.utils.set_random_seed(123)\n",
    "\n",
    "wine_data = pd.read_csv('data/winequality-red.csv', sep=';')\n",
    "\n",
    "ex_target = wine_data['quality']\n",
    "ex_features = wine_data.drop(['quality'], axis=1)\n",
    "\n",
    "ex_features = (ex_features - ex_features.min()) / (ex_features.max() - ex_features.min())  \n",
    "\n",
    "feat_train, feat_test, target_train, target_test = train_test_split(\n",
    "    ex_features, ex_target, test_size = 0.3, random_state=123)\n",
    "\n",
    "target_test = target_test.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0291751b-282c-45fe-a415-093525f5c0f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "(1.) Implement a feed-forward neural network with the following number \n",
    "of artificial neurons in the hidden layers: 1024, 512, 256, 128, 64. Use\n",
    "ReLU activations and the following hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050bb249-71f8-4e1d-93a3-10739446c3d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "batch_size = 64\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dca764-2c41-404b-a726-fba785dfaa5f",
   "metadata": {},
   "source": [
    "Put your result in the following cell and note:\n",
    "\n",
    "* Provide the model as ```ffnn_reg```, the history of the training in ```ffnn_reg_history```, and the predicted quality values in ```ex_pred_ffnn```. \n",
    "* The output in the regression setting is just 1-dimensional and no activation is required. \n",
    "* Use as the MSE loss ```keras.losses.MeanSquaredError()``` and do not use the ```metrics``` argument.\n",
    "* You can use the training set for ```validation_data```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5785b8-7299-452d-9557-49409ad5db3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a4463-f007-4cdf-9c62-2dac45a64c8e",
   "metadata": {},
   "source": [
    "Try to check the accuracy and loss during training with the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7d8bb-148b-430a-9443-1738630b1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ffnn_reg_history.history['loss'][1:], label='Training')\n",
    "plt.plot(ffnn_reg_history.history['val_loss'][1:], label='Test')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1af74-b695-4d37-b4d5-39b2d6b57e58",
   "metadata": {},
   "source": [
    "As we have done earlier, the following cell provides a \n",
    "data frame with an overview of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9e428-9ee6-4c3e-b6d5-bc616a9ee9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_test['NN_predicted_quality'] = np.around(ex_pred_ffnn, decimals=2)\n",
    "target_test['NN_absolut_deviation'] = np.around(abs(target_test['quality'] - target_test['NN_predicted_quality']), decimals=2)\n",
    "print(target_test)\n",
    "\n",
    "NN_pred_MSE = (target_test['NN_absolut_deviation']**2).mean()\n",
    "print(f\"\\nMean squared error of Feedforward Neural Network regression: {NN_pred_MSE:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
